{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrorostagno/tesis/blob/main/Tesis_nuevo2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qavi6v5_xrjI"
      },
      "outputs": [],
      "source": [
        "# Usar A100, es el que mejor resultados me dio hasta ahora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5djEVQr1xWU6",
        "outputId": "7a6aedd0-dc30-45d4-bd78-2747a168d4eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "files = os.listdir('/content/drive/Othercomputers/My Mac/Data/train/train/spoof/part_000')\n",
        "print(len(files))  # Si esto ya crashea, ya encontraste el cuello de botella\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "550-DVSV3SD2",
        "outputId": "d9c37914-fef9-41f8-8866-d26ddd66f2c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-1456961064.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/Othercomputers/My Mac/Data/train/train/spoof/part_000'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Si esto ya crashea, ya encontraste el cuello de botella\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/drive/Othercomputers/My Mac/Data/train/train/spoof/part_000'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3572\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3574\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLXmQkaSxz8D",
        "outputId": "21562244-86cf-410f-d6bc-4ca88be93039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Cach√©s en:     /content/drive/MyDrive/Tesis UTDT/cache\n",
            "üìÅ Resultados en: /content/drive/MyDrive/Tesis UTDT/experimentos/20250624-204246\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "dataset_path = '/content/drive/Othercomputers/My Mac/Data/train'\n",
        "\n",
        "# üóÇÔ∏è Carpetas base en Drive\n",
        "BASE_DIR = \"/content/drive/MyDrive/Tesis UTDT\"\n",
        "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
        "EXPERIMENTS_DIR = os.path.join(BASE_DIR, \"experimentos\")\n",
        "\n",
        "# ‚è±Ô∏è Nombre √∫nico del experimento basado en fecha y hora\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "RUN_DIR = os.path.join(EXPERIMENTS_DIR, timestamp)\n",
        "\n",
        "# üìÅ Crear carpetas si no existen\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Cach√©s en:     {CACHE_DIR}\")\n",
        "print(f\"üìÅ Resultados en: {RUN_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRN9PQq_0EoH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class CelebASpoofDatasetFromCSV(Dataset):\n",
        "    def __init__(self, csv_path, transform=None, max_samples=None,\n",
        "                 shuffle=True, seed=42, balanced=False):\n",
        "        \"\"\"\n",
        "        Dataset desde CSV con opci√≥n de balanceo.\n",
        "\n",
        "        CSV esperado con columnas: 'path', 'label'\n",
        "        - label: 0 (spoof), 1 (live)\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        df = pd.read_csv(csv_path)\n",
        "        random.seed(seed)\n",
        "\n",
        "        if balanced:\n",
        "            spoof_df = df[df['label'] == 0]\n",
        "            live_df  = df[df['label'] == 1]\n",
        "            min_len = min(len(spoof_df), len(live_df))\n",
        "\n",
        "            spoof_df = spoof_df.sample(n=min_len, random_state=seed)\n",
        "            live_df  = live_df.sample(n=min_len, random_state=seed)\n",
        "            df = pd.concat([spoof_df, live_df], ignore_index=True)\n",
        "\n",
        "            print(f\"‚öñÔ∏è Dataset balanceado: {min_len} spoof + {min_len} live = {2 * min_len} total\")\n",
        "\n",
        "        if shuffle:\n",
        "            df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        self.image_paths = df['path'].tolist()\n",
        "        self.labels = df['label'].tolist()\n",
        "\n",
        "        print(f\"üìÑ CSV cargado: {csv_path}\")\n",
        "        print(f\"üìä Total im√°genes cargadas: {len(self.image_paths)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q5n9D1s01lK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üìÅ Definici√≥n de rutas a cada partici√≥n del dataset (entrenamiento, validaci√≥n y test)\n",
        "train_data_dir = '/content/drive/Othercomputers/My Mac/Data/train/train'\n",
        "val_data_dir   = '/content/drive/Othercomputers/My Mac/Data/train/val'\n",
        "test_data_dir  = '/content/drive/Othercomputers/My Mac/Data/train/test'\n",
        "\n",
        "# üìê Dimensiones est√°ndar a las que se redimensionar√°n todas las im√°genes\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# ‚öôÔ∏è Tama√±o del batch para los DataLoaders\n",
        "batch_size = 32\n",
        "\n",
        "# üåÄ Transformaciones de preprocesamiento para cada conjunto de datos\n",
        "data_transforms = {\n",
        "    # 'train': transforms.Compose([\n",
        "    #     transforms.Resize((img_width, img_height)),              # Redimensiona la imagen a 224x224\n",
        "    #     transforms.RandomHorizontalFlip(),                       # Aplica flip horizontal aleatorio (augmentaci√≥n)\n",
        "    #     transforms.ToTensor(),                                   # Convierte PIL Image a tensor\n",
        "    #     transforms.Normalize([0.485, 0.456, 0.406],              # Normalizaci√≥n con media y desv√≠o est√°ndar de ImageNet\n",
        "    #                          [0.229, 0.224, 0.225])\n",
        "    # ]),\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((img_width + 32, img_height + 32)),    # Aumenta resoluci√≥n previa a crop\n",
        "        transforms.RandomResizedCrop((img_width, img_height), scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.02),\n",
        "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),\n",
        "        transforms.RandomApply([transforms.RandomRotation(10)], p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((img_width, img_height)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((img_width, img_height)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhXgn2Pn5wKp",
        "outputId": "f386bc6d-ddcb-4305-cfe4-467e06821a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öñÔ∏è Dataset balanceado: 106825 spoof + 106825 live = 213650 total\n",
            "üìÑ CSV cargado: /content/drive/Othercomputers/My Mac/Data/train/train_image_list.csv\n",
            "üìä Total im√°genes cargadas: 20000\n",
            "‚öñÔ∏è Dataset balanceado: 23863 spoof + 23863 live = 47726 total\n",
            "üìÑ CSV cargado: /content/drive/Othercomputers/My Mac/Data/train/val_image_list.csv\n",
            "üìä Total im√°genes cargadas: 5000\n"
          ]
        }
      ],
      "source": [
        "# Cantidad m√°xima de muestras por partici√≥n\n",
        "train_max_samples = 20000\n",
        "val_max_samples   = 5000\n",
        "test_max_samples  = 5000\n",
        "\n",
        "# üì¶ Dataset de entrenamiento desde CSV\n",
        "train_dataset = CelebASpoofDatasetFromCSV(\n",
        "    csv_path='/content/drive/Othercomputers/My Mac/Data/train/train_image_list.csv',\n",
        "    transform=data_transforms['train'],\n",
        "    max_samples=train_max_samples,\n",
        "    balanced=True\n",
        ")\n",
        "\n",
        "# üì¶ Dataset de validaci√≥n desde CSV\n",
        "val_dataset = CelebASpoofDatasetFromCSV(\n",
        "    csv_path='/content/drive/Othercomputers/My Mac/Data/train/val_image_list.csv',\n",
        "    transform=data_transforms['val'],\n",
        "    max_samples=val_max_samples,\n",
        "    balanced=True\n",
        ")\n",
        "\n",
        "# üì¶ Dataset de test desde CSV\n",
        "# test_dataset = CelebASpoofDatasetFromCSV(\n",
        "#     csv_path='/content/drive/Othercomputers/My Mac/Data/train/test_image_list.csv',\n",
        "#     transform=data_transforms['test'],\n",
        "#     max_samples=test_max_samples\n",
        "# )\n",
        "\n",
        "# üîÑ DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# test_loader = DataLoader(\n",
        "#     test_dataset,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=False,\n",
        "#     num_workers=8,\n",
        "#     pin_memory=True\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "def export_dataset_subset_to_local(dataset, split_name, output_base=\"/content/data\"):\n",
        "    \"\"\"\n",
        "    Copia las im√°genes de un dataset ya balanceado a almacenamiento local\n",
        "    y genera un nuevo CSV con paths locales.\n",
        "\n",
        "    Args:\n",
        "    - dataset: instancia de CelebASpoofDatasetFromCSV ya inicializada\n",
        "    - split_name: 'train', 'val' o 'test'\n",
        "    - output_base: carpeta donde se guardar√° todo\n",
        "    \"\"\"\n",
        "    local_paths, labels = [], []\n",
        "\n",
        "    for i, (img_path, label) in enumerate(zip(dataset.image_paths, dataset.labels)):\n",
        "        class_name = \"spoof\" if label == 0 else \"live\"\n",
        "        dst_dir = Path(output_base) / split_name / class_name\n",
        "        dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        src = Path(img_path)\n",
        "        dst = dst_dir / src.name\n",
        "\n",
        "        try:\n",
        "            shutil.copy(src, dst)\n",
        "            local_paths.append(str(dst))\n",
        "            labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error copiando {src}: {e}\")\n",
        "\n",
        "        if (i + 1) % 5000 == 0:\n",
        "            print(f\"‚úÖ {i+1} im√°genes procesadas...\")\n",
        "\n",
        "    df = pd.DataFrame({'path': local_paths, 'label': labels})\n",
        "    csv_path = Path(output_base) / f\"{split_name}_image_list_local.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nüìÑ CSV local guardado: {csv_path}\")\n",
        "    print(f\"üñºÔ∏è Total im√°genes copiadas: {len(df)}\")\n"
      ],
      "metadata": {
        "id": "6PCv4xobrpHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_dataset_subset_to_local(train_dataset, \"train\")  # üëà NUEVO\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "uLnUfjhtrxxE",
        "outputId": "0479e713-0a31-4fc3-97d5-c821d5b8c1b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è Error copiando /content/drive/Othercomputers/My Mac/Data/train/train/spoof/3802_123046.jpg: [Errno 5] Input/output error: '/content/drive/Othercomputers/My Mac/Data/train/train/spoof/3802_123046.jpg'\n",
            "‚ö†Ô∏è Error copiando /content/drive/Othercomputers/My Mac/Data/train/train/live/10049_119099.jpg: [Errno 5] Input/output error: '/content/drive/Othercomputers/My Mac/Data/train/train/live/10049_119099.jpg'\n",
            "‚ö†Ô∏è Error copiando /content/drive/Othercomputers/My Mac/Data/train/train/live/10004_024793.jpg: [Errno 5] Input/output error: '/content/drive/Othercomputers/My Mac/Data/train/train/live/10004_024793.jpg'\n",
            "‚ö†Ô∏è Error copiando /content/drive/Othercomputers/My Mac/Data/train/train/spoof/7742_411225.jpg: [Errno 5] Input/output error: '/content/drive/Othercomputers/My Mac/Data/train/train/spoof/7742_411225.jpg'\n",
            "‚ö†Ô∏è Error copiando /content/drive/Othercomputers/My Mac/Data/train/train/spoof/1095_103738.jpg: [Errno 5] Input/output error: '/content/drive/Othercomputers/My Mac/Data/train/train/spoof/1095_103738.jpg'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-4134229972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexport_dataset_subset_to_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# üëà NUEVO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-7-1854904202.py\u001b[0m in \u001b[0;36mexport_dataset_subset_to_local\u001b[0;34m(dataset, split_name, output_base)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mlocal_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    267\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0m_USE_CP_SENDFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                             \u001b[0m_fastcopy_sendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0m_GiveupOnFastCopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_fastcopy_sendfile\u001b[0;34m(fsrc, fdst)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# ...in oder to have a more informative exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(train_dataset.labels)\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print(f\"Distribuci√≥n de clases en entrenamiento: {dict(zip(unique, counts))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFdseMBWeQ3Q",
        "outputId": "bae24ca3-7826-453b-9c84-c7acca18d822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribuci√≥n de clases en entrenamiento: {np.int64(0): np.int64(9919), np.int64(1): np.int64(10081)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPA07Eq43g4p",
        "outputId": "d5f98ac4-6d1b-486a-bbc4-707c9608ddc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.6M/13.6M [00:00<00:00, 112MB/s] \n"
          ]
        }
      ],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ 1) MODELO  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from torchvision import models                     # ‚Üê ya lo usabas\n",
        "\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "num_ftrs = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_ftrs, 2)       # 2 clases\n",
        "\n",
        "# (opcional) descongela todo el backbone para fine-tuning completo\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62VUKKYL6vcy"
      },
      "outputs": [],
      "source": [
        "import os, json, torch, sklearn.metrics as metrics\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import amp                      # üëà  nueva forma\n",
        "from collections import OrderedDict\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "#  1) FUNCI√ìN DE P√âRDIDA ‚Äì FOCAL LOSS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=(0.25, 0.75), gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = torch.tensor(alpha)\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        # mueve alpha al device del tensor de logits\n",
        "        if self.alpha.device != logits.device:\n",
        "            self.alpha = self.alpha.to(logits.device)\n",
        "\n",
        "        ce_loss = self.ce(logits, targets)\n",
        "        pt      = torch.exp(-ce_loss)\n",
        "        at      = self.alpha.gather(0, targets)\n",
        "        loss    = at * (1 - pt) ** self.gamma * ce_loss\n",
        "        return loss.mean() if self.reduction == 'mean' else loss\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 2) BUCLE DE ENTRENAMIENTO (precisi√≥n mixta moderna)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def train_epoch(model, loader, optimizer, criterion, device, scaler):\n",
        "    model.train()\n",
        "    loss_sum, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with amp.autocast(device_type='cuda'):\n",
        "            logits = model(x)\n",
        "            loss   = criterion(logits, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += x.size(0)\n",
        "\n",
        "    return loss_sum / total, correct / total\n",
        "\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    loss_sum, correct, total = 0.0, 0, 0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with amp.autocast(device_type='cuda'):\n",
        "                logits = model(x)\n",
        "                loss   = criterion(logits, y)\n",
        "\n",
        "            loss_sum += loss.item() * x.size(0)\n",
        "            correct  += (logits.argmax(1) == y).sum().item()\n",
        "            total    += x.size(0)\n",
        "\n",
        "            y_true.extend(y.cpu().numpy())\n",
        "            y_pred.extend(logits.argmax(1).cpu().numpy())\n",
        "\n",
        "    report = metrics.classification_report(\n",
        "        y_true, y_pred, output_dict=True, zero_division=0\n",
        "    )\n",
        "    return loss_sum / total, correct / total, report\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 3) FUNCI√ìN PRINCIPAL DE ENTRENAMIENTO\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def train_model(\n",
        "        model, train_loader, val_loader,\n",
        "        criterion, optimizer, scheduler, device,\n",
        "        num_epochs=20, patience=6, checkpoint_interval=1, log_dir=None):\n",
        "\n",
        "    if log_dir is None:\n",
        "        log_dir = os.path.join(RUN_DIR, \"tensorboard\")\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    writer = SummaryWriter(log_dir)\n",
        "\n",
        "    # guardar config\n",
        "    json.dump(OrderedDict(\n",
        "        num_epochs=num_epochs,\n",
        "        patience=patience,\n",
        "        batch_size=train_loader.batch_size,\n",
        "        optimizer=type(optimizer).__name__,\n",
        "        lr_start=optimizer.param_groups[0][\"lr\"]\n",
        "    ), open(os.path.join(RUN_DIR, \"config.json\"), \"w\"), indent=2)\n",
        "\n",
        "    scaler          = amp.GradScaler()\n",
        "    best_val_loss   = float('inf')\n",
        "    wait            = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs}\\n\" + \"-\" * 20)\n",
        "\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer,\n",
        "                                      criterion, device, scaler)\n",
        "        val_loss, val_acc, report = validate_epoch(\n",
        "            model, val_loader, criterion, device)\n",
        "\n",
        "        scheduler.step()   # CosineAnnealingLR no necesita m√©tricas\n",
        "\n",
        "        print(f\"LR: {scheduler.get_last_lr()[0]:.6f}  \"\n",
        "              f\"TrainLoss: {tr_loss:.4f}  ValLoss: {val_loss:.4f}\")\n",
        "        print(f\"Val Acc: {val_acc:.4f}  F1_spoof: {report['0']['f1-score']:.3f}\")\n",
        "\n",
        "        # logs tensorboard\n",
        "        writer.add_scalars(\"Loss\", {\"train\": tr_loss, \"val\": val_loss}, epoch)\n",
        "        writer.add_scalars(\"Acc\",  {\"train\": tr_acc , \"val\": val_acc }, epoch)\n",
        "\n",
        "        # early-stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), os.path.join(RUN_DIR, \"best_model.pth\"))\n",
        "            print(\"‚úÖ  Nuevo mejor modelo guardado\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            print(f\"‚è≥  Sin mejora ({wait}/{patience})\")\n",
        "            if wait >= patience:\n",
        "                print(\"üõë  Early stopping\")\n",
        "                break\n",
        "\n",
        "        # checkpoint peri√≥dico\n",
        "        if epoch % checkpoint_interval == 0:\n",
        "            torch.save({\"epoch\": epoch,\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"optim\": optimizer.state_dict()},\n",
        "                       os.path.join(RUN_DIR, f\"ckpt_{epoch:02d}.pth\"))\n",
        "    writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNr4aLfGUprl"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model  = model.to(device)                          # tu MobileNetV2 ya modificado\n",
        "\n",
        "criterion  = FocalLoss()                           # ‚Üê nueva loss\n",
        "optimizer  = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "scheduler  = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NjeBDJpTUQQ",
        "outputId": "bb3df279-bf27-4924-ddf4-ad70430f3546"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# continua del bloque anterior, pero lo pongo aca para tenerlo separado\n",
        "# üß™ Ejecuci√≥n del entrenamiento (se asume que todo est√° definido)\n",
        "trained_model, *_ = train_model(\n",
        "        model, train_loader, val_loader,\n",
        "        criterion, optimizer, scheduler, device,\n",
        "        num_epochs=20, patience=6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "path = Path(\"/content/drive/Othercomputers/My Mac/Data/train/train/live/3681_282208.jpg\")\n",
        "print(\"¬øExiste?\", path.exists())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "TaegZv12k_5i",
        "outputId": "042edda0-1f6a-4451-e193-e3a6bc813941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \"\"\"\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/drive/Othercomputers/My Mac/Data/train/train/live/3681_282208.jpg'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3814020680.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/Othercomputers/My Mac/Data/train/train/live/3681_282208.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"¬øExiste?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36m_ignore_error\u001b[0;34m(exception)\u001b[0m\n\u001b[1;32m     36\u001b[0m     _WINERROR_CANT_RESOLVE_FILENAME)\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     return (getattr(exception, 'errno', None) in _IGNORED_ERRNOS or\n\u001b[1;32m     40\u001b[0m             getattr(exception, 'winerror', None) in _IGNORED_WINERRORS)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH9KuuHz72Uf",
        "outputId": "c0e358d0-66c4-48b3-f2b3-39fae3fd883e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Mejor modelo cargado desde RUN_DIR\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# üß† Cargar el mejor modelo desde el directorio de la corrida actual\n",
        "best_model = models.mobilenet_v2(pretrained=False)\n",
        "num_ftrs = best_model.classifier[1].in_features\n",
        "best_model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
        "best_model.load_state_dict(torch.load(os.path.join(RUN_DIR, 'best_model.pth')))\n",
        "best_model = best_model.to(device)\n",
        "print(\"‚úÖ Mejor modelo cargado desde RUN_DIR\")\n",
        "\n",
        "\n",
        "\n",
        "def test_model(model, test_loader, criterion, device, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    test_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Usar autocast para precisi√≥n mixta en la inferencia\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            total_samples += inputs.size(0)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = test_loss / total_samples\n",
        "    print(f'Test Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Convertir a arrays de numpy\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calcular matriz de confusi√≥n y reporte en formato dict\n",
        "    cm = metrics.confusion_matrix(all_labels, all_preds)\n",
        "    report = metrics.classification_report(all_labels, all_preds, target_names=[\"spoof\", \"live\"], zero_division=0)\n",
        "    report_dict = metrics.classification_report(all_labels, all_preds, target_names=[\"spoof\", \"live\"], output_dict=True, zero_division=0)\n",
        "\n",
        "    # Guardar m√©tricas como JSON\n",
        "    with open(os.path.join(output_dir, 'classification_report.json'), 'w') as f:\n",
        "        json.dump(report, f, indent=4)\n",
        "    print(\"üìÑ Reporte de clasificaci√≥n guardado en JSON.\")\n",
        "\n",
        "    print(\"Matriz de Confusi√≥n:\")\n",
        "    print(cm)\n",
        "    print(\"Reporte de Clasificaci√≥n:\")\n",
        "    print(report)\n",
        "\n",
        "    # Guardar matriz de confusi√≥n como imagen PNG\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"spoof\", \"live\"], yticklabels=[\"spoof\", \"live\"])\n",
        "    plt.xlabel(\"Predicci√≥n\")\n",
        "    plt.ylabel(\"Real\")\n",
        "    plt.title(\"Matriz de Confusi√≥n\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "    print(\"üñºÔ∏è Matriz de confusi√≥n guardada como imagen.\")\n",
        "\n",
        "    return avg_loss, report_dict, cm\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGz2agcmUXdn",
        "outputId": "bfafe47c-b0aa-4b99-b120-4b7b775f1500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset cargado desde cache (10000 im√°genes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-34-328312363.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.2539\n",
            "üìÑ Reporte de clasificaci√≥n guardado en JSON.\n",
            "Matriz de Confusi√≥n:\n",
            "[[1611 3714]\n",
            " [   1 4674]]\n",
            "Reporte de Clasificaci√≥n:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       spoof       1.00      0.30      0.46      5325\n",
            "        live       0.56      1.00      0.72      4675\n",
            "\n",
            "    accuracy                           0.63     10000\n",
            "   macro avg       0.78      0.65      0.59     10000\n",
            "weighted avg       0.79      0.63      0.58     10000\n",
            "\n",
            "üñºÔ∏è Matriz de confusi√≥n guardada como imagen.\n"
          ]
        }
      ],
      "source": [
        "# Continuacion del bloque anterior. Testeo del modelo en otro dataset\n",
        "test_data_dir2 = '/content/drive/Othercomputers/My Mac/Data/test'\n",
        "\n",
        "test_dataset2 = CelebASpoofDataset(\n",
        "    root_dir=test_data_dir2,\n",
        "    transform=data_transforms['test'],\n",
        "    cache_file=os.path.join(CACHE_DIR, 'test_dataset_cache_full.pkl'),\n",
        "    max_samples=5000\n",
        "    # max_subjects=300\n",
        ")\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "\n",
        "# Ejemplo de uso:\n",
        "# test_loss, test_report, test_cm = test_model(model, test_loader2, criterion, device)\n",
        "test_loss, test_report, test_cm = test_model(best_model, test_loader2, criterion, device, output_dir=RUN_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW8B2HBXX5-M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def log_experiment_summary(run_dir,\n",
        "                           train_loss, train_acc,\n",
        "                           val_loss, val_acc, val_report,\n",
        "                           test_loss, test_report,\n",
        "                           train_max_samples, val_max_samples, test_max_samples):\n",
        "    summary_path = os.path.join(EXPERIMENTS_DIR, \"all_runs.csv\")\n",
        "    run_name = os.path.basename(run_dir)\n",
        "\n",
        "    # Utilidad segura para extraer m√©tricas, aunque una clase no est√© presente\n",
        "    def get_metric(report, class_name, metric):\n",
        "        return round(report.get(class_name, {}).get(metric, 0.0), 4)\n",
        "\n",
        "    summary_data = {\n",
        "        \"run_name\": run_name,\n",
        "\n",
        "        # M√©tricas de entrenamiento\n",
        "        \"train_loss\": round(train_loss, 4),\n",
        "        \"train_acc\": round(train_acc.item(), 4),\n",
        "\n",
        "        # M√©tricas de validaci√≥n\n",
        "        \"val_loss\": round(val_loss, 4),\n",
        "        \"val_acc\": round(val_acc.item(), 4),\n",
        "        \"f1_macro_val\": get_metric(val_report, \"macro avg\", \"f1-score\"),\n",
        "        \"precision_spoof_val\": get_metric(val_report, \"0\", \"precision\"),\n",
        "        \"recall_spoof_val\": get_metric(val_report, \"0\", \"recall\"),\n",
        "        \"f1_spoof_val\": get_metric(val_report, \"0\", \"f1-score\"),\n",
        "        \"precision_live_val\": get_metric(val_report, \"1\", \"precision\"),\n",
        "        \"recall_live_val\": get_metric(val_report, \"1\", \"recall\"),\n",
        "        \"f1_live_val\": get_metric(val_report, \"1\", \"f1-score\"),\n",
        "\n",
        "        # M√©tricas de test final\n",
        "        \"test_loss\": round(test_loss, 4),\n",
        "        \"f1_macro_test\": get_metric(test_report, \"macro avg\", \"f1-score\"),\n",
        "        \"precision_spoof_test\": get_metric(test_report, \"spoof\", \"precision\"),\n",
        "        \"recall_spoof_test\": get_metric(test_report, \"spoof\", \"recall\"),\n",
        "        \"f1_spoof_test\": get_metric(test_report, \"spoof\", \"f1-score\"),\n",
        "        \"precision_live_test\": get_metric(test_report, \"live\", \"precision\"),\n",
        "        \"recall_live_test\": get_metric(test_report, \"live\", \"recall\"),\n",
        "        \"f1_live_test\": get_metric(test_report, \"live\", \"f1-score\"),\n",
        "\n",
        "        # Info de dataset\n",
        "        \"train_max_samples\": train_max_samples,\n",
        "        \"val_max_samples\": val_max_samples,\n",
        "        \"test_max_samples\": test_max_samples\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', header=False, index=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"üìù Resumen de la corrida guardado en {summary_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAsTob5fZ_iW",
        "outputId": "b93c053e-b5cd-4f30-b8a6-9c30f7f4579d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'spoof': {'precision': 0.9924892703862661,\n",
              "  'recall': 0.3474178403755869,\n",
              "  'f1-score': 0.514675198219502,\n",
              "  'support': 5325.0},\n",
              " 'live': {'precision': 0.5728859390363815,\n",
              "  'recall': 0.9970053475935828,\n",
              "  'f1-score': 0.7276559206931543,\n",
              "  'support': 4675.0},\n",
              " 'accuracy': 0.6511,\n",
              " 'macro avg': {'precision': 0.7826876047113238,\n",
              "  'recall': 0.6722115939845849,\n",
              "  'f1-score': 0.6211655594563281,\n",
              "  'support': 10000.0},\n",
              " 'weighted avg': {'precision': 0.796324712980195,\n",
              "  'recall': 0.6511,\n",
              "  'f1-score': 0.6142436859759345,\n",
              "  'support': 10000.0}}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE0EMB-aYERA",
        "outputId": "03344fe2-9d90-4424-b212-0779525a25ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Resumen de la corrida guardado en /content/drive/MyDrive/Tesis UTDT/experimentos/all_runs.csv\n"
          ]
        }
      ],
      "source": [
        "log_experiment_summary(\n",
        "    run_dir=RUN_DIR,\n",
        "    train_loss=train_loss,\n",
        "    train_acc=train_acc,\n",
        "    val_loss=val_loss,\n",
        "    val_acc=val_acc,\n",
        "    val_report=val_report,\n",
        "    test_loss=test_loss,\n",
        "    test_report=test_report,\n",
        "    train_max_samples=train_max_samples,\n",
        "    val_max_samples=val_max_samples,\n",
        "    test_max_samples=test_max_samples\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBXaRogcnMSE",
        "outputId": "1cef2aeb-a265-435b-aeff-cd5948bb90b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR ‚Üí Pred: 1 | Real: 0 | Path: /content/drive/Othercomputers/My Mac/Data/test/5028/spoof/538544.png\n"
          ]
        }
      ],
      "source": [
        "# Mostrar 10 predicciones incorrectas con su path\n",
        "for i, (img, label) in enumerate(test_dataset2):\n",
        "    pred = model(img.unsqueeze(0).to(device)).argmax(dim=1).item()\n",
        "    if pred != label:\n",
        "        print(f\"ERROR ‚Üí Pred: {pred} | Real: {label} | Path: {test_dataset2.image_paths[i]}\")\n",
        "        if i > 10: break\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOTQF1vfLJqsknuUMa/Ejtc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}