{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrorostagno/tesis/blob/main/Tesis_nuevo2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qavi6v5_xrjI"
      },
      "outputs": [],
      "source": [
        "# Usar A100, es el que mejor resultados me dio hasta ahora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5djEVQr1xWU6",
        "outputId": "82358bdb-d310-4ddb-ff4c-2836aeca7971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "550-DVSV3SD2",
        "outputId": "b6643878-b2b7-41e8-c778-d1939a1e8e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "files = os.listdir('/content/drive/Othercomputers/My Mac/Data/train/train/spoof/part_000')\n",
        "print(len(files))  # Si esto ya crashea, ya encontraste el cuello de botella\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLXmQkaSxz8D",
        "outputId": "65786290-d345-481f-b8a8-d54c3af7f979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ CachÃ©s en:     /content/drive/MyDrive/Tesis UTDT/cache\n",
            "ğŸ“ Resultados en: /content/drive/MyDrive/Tesis UTDT/experimentos/20250701-210846\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "dataset_path = '/content/drive/Othercomputers/My Mac/Data/train'\n",
        "\n",
        "# ğŸ—‚ï¸ Carpetas base en Drive\n",
        "BASE_DIR = \"/content/drive/MyDrive/Tesis UTDT\"\n",
        "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
        "EXPERIMENTS_DIR = os.path.join(BASE_DIR, \"experimentos\")\n",
        "\n",
        "# â±ï¸ Nombre Ãºnico del experimento basado en fecha y hora\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "RUN_DIR = os.path.join(EXPERIMENTS_DIR, timestamp)\n",
        "\n",
        "# ğŸ“ Crear carpetas si no existen\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"ğŸ“ CachÃ©s en:     {CACHE_DIR}\")\n",
        "print(f\"ğŸ“ Resultados en: {RUN_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRN9PQq_0EoH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class CelebASpoofDatasetFromCSV(Dataset):\n",
        "    def __init__(self, csv_path, transform=None, max_samples=None,\n",
        "                 shuffle=True, seed=42, balanced=False):\n",
        "        \"\"\"\n",
        "        Dataset desde CSV con opciÃ³n de balanceo.\n",
        "\n",
        "        CSV esperado con columnas: 'path', 'label'\n",
        "        - label: 0 (spoof), 1 (live)\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        df = pd.read_csv(csv_path)\n",
        "        random.seed(seed)\n",
        "\n",
        "        if balanced:\n",
        "            spoof_df = df[df['label'] == 0]\n",
        "            live_df  = df[df['label'] == 1]\n",
        "            min_len = min(len(spoof_df), len(live_df))\n",
        "\n",
        "            spoof_df = spoof_df.sample(n=min_len, random_state=seed)\n",
        "            live_df  = live_df.sample(n=min_len, random_state=seed)\n",
        "            df = pd.concat([spoof_df, live_df], ignore_index=True)\n",
        "\n",
        "            print(f\"âš–ï¸ Dataset balanceado: {min_len} spoof + {min_len} live = {2 * min_len} total\")\n",
        "\n",
        "        if shuffle:\n",
        "            df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        self.image_paths = df['path'].tolist()\n",
        "        self.labels = df['label'].tolist()\n",
        "\n",
        "        print(f\"ğŸ“„ CSV cargado: {csv_path}\")\n",
        "        print(f\"ğŸ“Š Total imÃ¡genes cargadas: {len(self.image_paths)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q5n9D1s01lK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ğŸ“ DefiniciÃ³n de rutas a cada particiÃ³n del dataset (entrenamiento, validaciÃ³n y test)\n",
        "train_data_dir = '/content/drive/Othercomputers/My Mac/Data/train/train'\n",
        "val_data_dir   = '/content/drive/Othercomputers/My Mac/Data/train/val'\n",
        "test_data_dir  = '/content/drive/Othercomputers/My Mac/Data/train/test'\n",
        "\n",
        "# ğŸ“ Dimensiones estÃ¡ndar a las que se redimensionarÃ¡n todas las imÃ¡genes\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# âš™ï¸ TamaÃ±o del batch para los DataLoaders\n",
        "batch_size = 32\n",
        "\n",
        "# ğŸŒ€ Transformaciones de preprocesamiento para cada conjunto de datos\n",
        "data_transforms = {\n",
        "    # 'train': transforms.Compose([\n",
        "    #     transforms.Resize((img_width, img_height)),              # Redimensiona la imagen a 224x224\n",
        "    #     transforms.RandomHorizontalFlip(),                       # Aplica flip horizontal aleatorio (augmentaciÃ³n)\n",
        "    #     transforms.ToTensor(),                                   # Convierte PIL Image a tensor\n",
        "    #     transforms.Normalize([0.485, 0.456, 0.406],              # NormalizaciÃ³n con media y desvÃ­o estÃ¡ndar de ImageNet\n",
        "    #                          [0.229, 0.224, 0.225])\n",
        "    # ]),\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((img_width + 32, img_height + 32)),    # Aumenta resoluciÃ³n previa a crop\n",
        "        transforms.RandomResizedCrop((img_width, img_height), scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.02),\n",
        "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),\n",
        "        transforms.RandomApply([transforms.RandomRotation(10)], p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((img_width, img_height)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((img_width, img_height)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhXgn2Pn5wKp",
        "outputId": "acf0c1fb-7e70-4bf5-d610-9ef0adac2495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš–ï¸ Dataset balanceado: 106825 spoof + 106825 live = 213650 total\n",
            "ğŸ“„ CSV cargado: /content/drive/Othercomputers/My Mac/Data/train/train_image_list.csv\n",
            "ğŸ“Š Total imÃ¡genes cargadas: 100000\n",
            "âš–ï¸ Dataset balanceado: 23863 spoof + 23863 live = 47726 total\n",
            "ğŸ“„ CSV cargado: /content/drive/Othercomputers/My Mac/Data/train/val_image_list.csv\n",
            "ğŸ“Š Total imÃ¡genes cargadas: 20000\n"
          ]
        }
      ],
      "source": [
        "# Cantidad mÃ¡xima de muestras por particiÃ³n\n",
        "train_max_samples = 100000\n",
        "val_max_samples   = 20000\n",
        "test_max_samples  = 5000\n",
        "\n",
        "# ğŸ“¦ Dataset de entrenamiento desde CSV\n",
        "train_dataset = CelebASpoofDatasetFromCSV(\n",
        "    csv_path='/content/drive/Othercomputers/My Mac/Data/train/train_image_list.csv',\n",
        "    transform=data_transforms['train'],\n",
        "    max_samples=train_max_samples,\n",
        "    balanced=True\n",
        ")\n",
        "\n",
        "# ğŸ“¦ Dataset de validaciÃ³n desde CSV\n",
        "val_dataset = CelebASpoofDatasetFromCSV(\n",
        "    csv_path='/content/drive/Othercomputers/My Mac/Data/train/val_image_list.csv',\n",
        "    transform=data_transforms['val'],\n",
        "    max_samples=val_max_samples,\n",
        "    balanced=True\n",
        ")\n",
        "\n",
        "# ğŸ“¦ Dataset de test desde CSV\n",
        "# test_dataset = CelebASpoofDatasetFromCSV(\n",
        "#     csv_path='/content/drive/Othercomputers/My Mac/Data/train/test_image_list.csv',\n",
        "#     transform=data_transforms['test'],\n",
        "#     max_samples=test_max_samples\n",
        "# )\n",
        "\n",
        "# ğŸ”„ DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=10,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=10,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# test_loader = DataLoader(\n",
        "#     test_dataset,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=False,\n",
        "#     num_workers=8,\n",
        "#     pin_memory=True\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFdseMBWeQ3Q",
        "outputId": "e14e166f-b005-4e34-ec42-985414bd05a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DistribuciÃ³n de clases en entrenamiento: {np.int64(0): np.int64(49785), np.int64(1): np.int64(50215)}\n"
          ]
        }
      ],
      "source": [
        "labels = np.array(train_dataset.labels)\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print(f\"DistribuciÃ³n de clases en entrenamiento: {dict(zip(unique, counts))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPA07Eq43g4p",
        "outputId": "5a5c4012-c34b-4eb5-e9f7-3df531a9cbf2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# â”€â”€â”€ 1) MODELO  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from torchvision import models                     # â† ya lo usabas\n",
        "\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "num_ftrs = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_ftrs, 2)       # 2 clases\n",
        "\n",
        "# (opcional) descongela todo el backbone para fine-tuning completo\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62VUKKYL6vcy"
      },
      "outputs": [],
      "source": [
        "import os, json, torch, sklearn.metrics as metrics\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import amp                      # ğŸ‘ˆ  nueva forma\n",
        "from collections import OrderedDict\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#  1) FUNCIÃ“N DE PÃ‰RDIDA â€“ FOCAL LOSS\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=(0.25, 0.75), gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = torch.tensor(alpha)\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        # mueve alpha al device del tensor de logits\n",
        "        if self.alpha.device != logits.device:\n",
        "            self.alpha = self.alpha.to(logits.device)\n",
        "\n",
        "        ce_loss = self.ce(logits, targets)\n",
        "        pt      = torch.exp(-ce_loss)\n",
        "        at      = self.alpha.gather(0, targets)\n",
        "        loss    = at * (1 - pt) ** self.gamma * ce_loss\n",
        "        return loss.mean() if self.reduction == 'mean' else loss\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2) BUCLE DE ENTRENAMIENTO (precisiÃ³n mixta moderna)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def train_epoch(model, loader, optimizer, criterion, device, scaler):\n",
        "    model.train()\n",
        "    loss_sum, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with amp.autocast(device_type='cuda'):\n",
        "            logits = model(x)\n",
        "            loss   = criterion(logits, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += x.size(0)\n",
        "\n",
        "    return loss_sum / total, correct / total\n",
        "\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    loss_sum, correct, total = 0.0, 0, 0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with amp.autocast(device_type='cuda'):\n",
        "                logits = model(x)\n",
        "                loss   = criterion(logits, y)\n",
        "\n",
        "            loss_sum += loss.item() * x.size(0)\n",
        "            correct  += (logits.argmax(1) == y).sum().item()\n",
        "            total    += x.size(0)\n",
        "\n",
        "            y_true.extend(y.cpu().numpy())\n",
        "            y_pred.extend(logits.argmax(1).cpu().numpy())\n",
        "\n",
        "    report = metrics.classification_report(\n",
        "        y_true, y_pred, output_dict=True, zero_division=0\n",
        "    )\n",
        "    return loss_sum / total, correct / total, report\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 3) FUNCIÃ“N PRINCIPAL DE ENTRENAMIENTO\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def train_model(\n",
        "        model, train_loader, val_loader,\n",
        "        criterion, optimizer, scheduler, device,\n",
        "        num_epochs=20, patience=6, checkpoint_interval=1, log_dir=None):\n",
        "\n",
        "    if log_dir is None:\n",
        "        log_dir = os.path.join(RUN_DIR, \"tensorboard\")\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    writer = SummaryWriter(log_dir)\n",
        "\n",
        "    # guardar config\n",
        "    json.dump(OrderedDict(\n",
        "        num_epochs=num_epochs,\n",
        "        patience=patience,\n",
        "        batch_size=train_loader.batch_size,\n",
        "        optimizer=type(optimizer).__name__,\n",
        "        lr_start=optimizer.param_groups[0][\"lr\"]\n",
        "    ), open(os.path.join(RUN_DIR, \"config.json\"), \"w\"), indent=2)\n",
        "\n",
        "    scaler          = amp.GradScaler()\n",
        "    best_val_loss   = float('inf')\n",
        "    wait            = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs}\\n\" + \"-\" * 20)\n",
        "\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer,\n",
        "                                      criterion, device, scaler)\n",
        "        val_loss, val_acc, report = validate_epoch(\n",
        "            model, val_loader, criterion, device)\n",
        "\n",
        "        scheduler.step()   # CosineAnnealingLR no necesita mÃ©tricas\n",
        "\n",
        "        print(f\"LR: {scheduler.get_last_lr()[0]:.6f}  \"\n",
        "              f\"TrainLoss: {tr_loss:.4f}  ValLoss: {val_loss:.4f}\")\n",
        "        print(f\"Val Acc: {val_acc:.4f}  F1_spoof: {report['0']['f1-score']:.3f}\")\n",
        "\n",
        "        # logs tensorboard\n",
        "        writer.add_scalars(\"Loss\", {\"train\": tr_loss, \"val\": val_loss}, epoch)\n",
        "        writer.add_scalars(\"Acc\",  {\"train\": tr_acc , \"val\": val_acc }, epoch)\n",
        "\n",
        "        # early-stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), os.path.join(RUN_DIR, \"best_model.pth\"))\n",
        "            print(\"âœ…  Nuevo mejor modelo guardado\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            print(f\"â³  Sin mejora ({wait}/{patience})\")\n",
        "            if wait >= patience:\n",
        "                print(\"ğŸ›‘  Early stopping\")\n",
        "                break\n",
        "\n",
        "        # checkpoint periÃ³dico\n",
        "        if epoch % checkpoint_interval == 0:\n",
        "            torch.save({\"epoch\": epoch,\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"optim\": optimizer.state_dict()},\n",
        "                       os.path.join(RUN_DIR, f\"ckpt_{epoch:02d}.pth\"))\n",
        "    writer.close()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNr4aLfGUprl"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model  = model.to(device)                          # tu MobileNetV2 ya modificado\n",
        "\n",
        "criterion  = FocalLoss()                           # â† nueva loss\n",
        "optimizer  = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "scheduler  = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NjeBDJpTUQQ",
        "outputId": "d59e1fd8-f981-44dc-960d-7cc3c995c5f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "--------------------\n",
            "LR: 0.000497  TrainLoss: 0.0099  ValLoss: 0.0027\n",
            "Val Acc: 0.9922  F1_spoof: 0.992\n",
            "âœ…  Nuevo mejor modelo guardado\n",
            "\n",
            "Epoch 2/10\n",
            "--------------------\n",
            "LR: 0.000488  TrainLoss: 0.0064  ValLoss: 0.0021\n",
            "Val Acc: 0.9936  F1_spoof: 0.994\n",
            "âœ…  Nuevo mejor modelo guardado\n",
            "\n",
            "Epoch 3/10\n",
            "--------------------\n",
            "LR: 0.000473  TrainLoss: 0.0052  ValLoss: 0.0026\n",
            "Val Acc: 0.9914  F1_spoof: 0.991\n",
            "â³  Sin mejora (1/6)\n",
            "\n",
            "Epoch 4/10\n",
            "--------------------\n",
            "LR: 0.000452  TrainLoss: 0.0042  ValLoss: 0.0030\n",
            "Val Acc: 0.9939  F1_spoof: 0.994\n",
            "â³  Sin mejora (2/6)\n",
            "\n",
            "Epoch 5/10\n",
            "--------------------\n",
            "LR: 0.000427  TrainLoss: 0.0038  ValLoss: 0.0018\n",
            "Val Acc: 0.9947  F1_spoof: 0.995\n",
            "âœ…  Nuevo mejor modelo guardado\n",
            "\n",
            "Epoch 6/10\n",
            "--------------------\n",
            "LR: 0.000397  TrainLoss: 0.0034  ValLoss: 0.0012\n",
            "Val Acc: 0.9969  F1_spoof: 0.997\n",
            "âœ…  Nuevo mejor modelo guardado\n",
            "\n",
            "Epoch 7/10\n",
            "--------------------\n",
            "LR: 0.000363  TrainLoss: 0.0029  ValLoss: 0.0020\n",
            "Val Acc: 0.9936  F1_spoof: 0.994\n",
            "â³  Sin mejora (1/6)\n",
            "\n",
            "Epoch 8/10\n",
            "--------------------\n",
            "LR: 0.000327  TrainLoss: 0.0025  ValLoss: 0.0012\n",
            "Val Acc: 0.9966  F1_spoof: 0.997\n",
            "â³  Sin mejora (2/6)\n",
            "\n",
            "Epoch 9/10\n",
            "--------------------\n",
            "LR: 0.000289  TrainLoss: 0.0021  ValLoss: 0.0014\n",
            "Val Acc: 0.9964  F1_spoof: 0.996\n",
            "â³  Sin mejora (3/6)\n",
            "\n",
            "Epoch 10/10\n",
            "--------------------\n",
            "LR: 0.000250  TrainLoss: 0.0019  ValLoss: 0.0015\n",
            "Val Acc: 0.9956  F1_spoof: 0.996\n",
            "â³  Sin mejora (4/6)\n"
          ]
        }
      ],
      "source": [
        "# continua del bloque anterior, pero lo pongo aca para tenerlo separado\n",
        "# ğŸ§ª EjecuciÃ³n del entrenamiento (se asume que todo estÃ¡ definido)\n",
        "trained_model = train_model(\n",
        "        model, train_loader, val_loader,\n",
        "        criterion, optimizer, scheduler, device,\n",
        "        num_epochs=10, patience=6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH9KuuHz72Uf",
        "outputId": "e8edc794-7530-4ff0-c896-f378252a612b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Mejor modelo cargado desde RUN_DIR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# ğŸ§  Cargar el mejor modelo desde el directorio de la corrida actual\n",
        "best_model = models.mobilenet_v2(pretrained=False)\n",
        "num_ftrs = best_model.classifier[1].in_features\n",
        "best_model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
        "best_model.load_state_dict(torch.load(os.path.join(RUN_DIR, 'best_model.pth')))\n",
        "best_model = best_model.to(device)\n",
        "print(\"âœ… Mejor modelo cargado desde RUN_DIR\")\n",
        "\n",
        "\n",
        "\n",
        "def test_model(model, test_loader, criterion, device, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    test_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Usar autocast para precisiÃ³n mixta en la inferencia\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            total_samples += inputs.size(0)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = test_loss / total_samples\n",
        "    print(f'Test Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Convertir a arrays de numpy\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calcular matriz de confusiÃ³n y reporte en formato dict\n",
        "    cm = metrics.confusion_matrix(all_labels, all_preds)\n",
        "    report = metrics.classification_report(all_labels, all_preds, target_names=[\"spoof\", \"live\"], zero_division=0)\n",
        "    report_dict = metrics.classification_report(all_labels, all_preds, target_names=[\"spoof\", \"live\"], output_dict=True, zero_division=0)\n",
        "\n",
        "    # Guardar mÃ©tricas como JSON\n",
        "    with open(os.path.join(output_dir, 'classification_report.json'), 'w') as f:\n",
        "        json.dump(report, f, indent=4)\n",
        "    print(\"ğŸ“„ Reporte de clasificaciÃ³n guardado en JSON.\")\n",
        "\n",
        "    print(\"Matriz de ConfusiÃ³n:\")\n",
        "    print(cm)\n",
        "    print(\"Reporte de ClasificaciÃ³n:\")\n",
        "    print(report)\n",
        "\n",
        "    # Guardar matriz de confusiÃ³n como imagen PNG\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"spoof\", \"live\"], yticklabels=[\"spoof\", \"live\"])\n",
        "    plt.xlabel(\"PredicciÃ³n\")\n",
        "    plt.ylabel(\"Real\")\n",
        "    plt.title(\"Matriz de ConfusiÃ³n\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "    print(\"ğŸ–¼ï¸ Matriz de confusiÃ³n guardada como imagen.\")\n",
        "\n",
        "    return avg_loss, report_dict, cm\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGz2agcmUXdn",
        "outputId": "7869561d-6cf1-4b67-92cb-39b9bf1aec44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“„ CSV cargado: /content/drive/Othercomputers/My Mac/Data/train/test_image_list.csv\n",
            "ğŸ“Š Total imÃ¡genes cargadas: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-40-328312363.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0022\n",
            "ğŸ“„ Reporte de clasificaciÃ³n guardado en JSON.\n",
            "Matriz de ConfusiÃ³n:\n",
            "[[7259   96]\n",
            " [  11 2634]]\n",
            "Reporte de ClasificaciÃ³n:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       spoof       1.00      0.99      0.99      7355\n",
            "        live       0.96      1.00      0.98      2645\n",
            "\n",
            "    accuracy                           0.99     10000\n",
            "   macro avg       0.98      0.99      0.99     10000\n",
            "weighted avg       0.99      0.99      0.99     10000\n",
            "\n",
            "ğŸ–¼ï¸ Matriz de confusiÃ³n guardada como imagen.\n"
          ]
        }
      ],
      "source": [
        "# Continuacion del bloque anterior. Testeo del modelo en otro dataset\n",
        "test_data_dir2 = '/content/drive/Othercomputers/My Mac/Data/test'\n",
        "\n",
        "\n",
        "# ğŸ“¦ Dataset de test desde CSV\n",
        "test_dataset = CelebASpoofDatasetFromCSV(\n",
        "    csv_path='/content/drive/Othercomputers/My Mac/Data/train/test_image_list.csv',\n",
        "    transform=data_transforms['test'],\n",
        "    max_samples=10000\n",
        ")\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "test_loader2 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "\n",
        "# Ejemplo de uso:\n",
        "# test_loss, test_report, test_cm = test_model(model, test_loader2, criterion, device)\n",
        "test_loss, test_report, test_cm = test_model(best_model, test_loader2, criterion, device, output_dir=RUN_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDcmFDyOvqdc",
        "outputId": "1f2ce79f-ceec-45f6-da78-d11f5bee8861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“„ CSV cargado: /content/drive/Othercomputers/My Mac/Data/train/test2_image_list.csv\n",
            "ğŸ“Š Total imÃ¡genes cargadas: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-40-328312363.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1780\n",
            "ğŸ“„ Reporte de clasificaciÃ³n guardado en JSON.\n",
            "Matriz de ConfusiÃ³n:\n",
            "[[4081 2991]\n",
            " [   3 2925]]\n",
            "Reporte de ClasificaciÃ³n:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       spoof       1.00      0.58      0.73      7072\n",
            "        live       0.49      1.00      0.66      2928\n",
            "\n",
            "    accuracy                           0.70     10000\n",
            "   macro avg       0.75      0.79      0.70     10000\n",
            "weighted avg       0.85      0.70      0.71     10000\n",
            "\n",
            "ğŸ–¼ï¸ Matriz de confusiÃ³n guardada como imagen.\n"
          ]
        }
      ],
      "source": [
        "# Continuacion del bloque anterior. Testeo del modelo en otro dataset\n",
        "test_data_dir2 = '/content/drive/Othercomputers/My Mac/Data/test'\n",
        "\n",
        "\n",
        "# ğŸ“¦ Dataset de test desde CSV\n",
        "test_dataset2 = CelebASpoofDatasetFromCSV(\n",
        "    csv_path='/content/drive/Othercomputers/My Mac/Data/train/test2_image_list.csv',\n",
        "    transform=data_transforms['test'],\n",
        "    max_samples=10000\n",
        ")\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "\n",
        "# Ejemplo de uso:\n",
        "# test_loss, test_report, test_cm = test_model(model, test_loader2, criterion, device)\n",
        "test_loss, test_report, test_cm = test_model(best_model, test_loader2, criterion, device, output_dir=RUN_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW8B2HBXX5-M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def log_experiment_summary(run_dir,\n",
        "                           train_loss, train_acc,\n",
        "                           val_loss, val_acc, val_report,\n",
        "                           test_loss, test_report,\n",
        "                           train_max_samples, val_max_samples, test_max_samples):\n",
        "    summary_path = os.path.join(EXPERIMENTS_DIR, \"all_runs.csv\")\n",
        "    run_name = os.path.basename(run_dir)\n",
        "\n",
        "    # Utilidad segura para extraer mÃ©tricas, aunque una clase no estÃ© presente\n",
        "    def get_metric(report, class_name, metric):\n",
        "        return round(report.get(class_name, {}).get(metric, 0.0), 4)\n",
        "\n",
        "    summary_data = {\n",
        "        \"run_name\": run_name,\n",
        "\n",
        "        # MÃ©tricas de entrenamiento\n",
        "        \"train_loss\": round(train_loss, 4),\n",
        "        \"train_acc\": round(train_acc.item(), 4),\n",
        "\n",
        "        # MÃ©tricas de validaciÃ³n\n",
        "        \"val_loss\": round(val_loss, 4),\n",
        "        \"val_acc\": round(val_acc.item(), 4),\n",
        "        \"f1_macro_val\": get_metric(val_report, \"macro avg\", \"f1-score\"),\n",
        "        \"precision_spoof_val\": get_metric(val_report, \"0\", \"precision\"),\n",
        "        \"recall_spoof_val\": get_metric(val_report, \"0\", \"recall\"),\n",
        "        \"f1_spoof_val\": get_metric(val_report, \"0\", \"f1-score\"),\n",
        "        \"precision_live_val\": get_metric(val_report, \"1\", \"precision\"),\n",
        "        \"recall_live_val\": get_metric(val_report, \"1\", \"recall\"),\n",
        "        \"f1_live_val\": get_metric(val_report, \"1\", \"f1-score\"),\n",
        "\n",
        "        # MÃ©tricas de test final\n",
        "        \"test_loss\": round(test_loss, 4),\n",
        "        \"f1_macro_test\": get_metric(test_report, \"macro avg\", \"f1-score\"),\n",
        "        \"precision_spoof_test\": get_metric(test_report, \"spoof\", \"precision\"),\n",
        "        \"recall_spoof_test\": get_metric(test_report, \"spoof\", \"recall\"),\n",
        "        \"f1_spoof_test\": get_metric(test_report, \"spoof\", \"f1-score\"),\n",
        "        \"precision_live_test\": get_metric(test_report, \"live\", \"precision\"),\n",
        "        \"recall_live_test\": get_metric(test_report, \"live\", \"recall\"),\n",
        "        \"f1_live_test\": get_metric(test_report, \"live\", \"f1-score\"),\n",
        "\n",
        "        # Info de dataset\n",
        "        \"train_max_samples\": train_max_samples,\n",
        "        \"val_max_samples\": val_max_samples,\n",
        "        \"test_max_samples\": test_max_samples\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', header=False, index=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"ğŸ“ Resumen de la corrida guardado en {summary_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAsTob5fZ_iW",
        "outputId": "b93c053e-b5cd-4f30-b8a6-9c30f7f4579d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'spoof': {'precision': 0.9924892703862661,\n",
              "  'recall': 0.3474178403755869,\n",
              "  'f1-score': 0.514675198219502,\n",
              "  'support': 5325.0},\n",
              " 'live': {'precision': 0.5728859390363815,\n",
              "  'recall': 0.9970053475935828,\n",
              "  'f1-score': 0.7276559206931543,\n",
              "  'support': 4675.0},\n",
              " 'accuracy': 0.6511,\n",
              " 'macro avg': {'precision': 0.7826876047113238,\n",
              "  'recall': 0.6722115939845849,\n",
              "  'f1-score': 0.6211655594563281,\n",
              "  'support': 10000.0},\n",
              " 'weighted avg': {'precision': 0.796324712980195,\n",
              "  'recall': 0.6511,\n",
              "  'f1-score': 0.6142436859759345,\n",
              "  'support': 10000.0}}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE0EMB-aYERA",
        "outputId": "03344fe2-9d90-4424-b212-0779525a25ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ Resumen de la corrida guardado en /content/drive/MyDrive/Tesis UTDT/experimentos/all_runs.csv\n"
          ]
        }
      ],
      "source": [
        "log_experiment_summary(\n",
        "    run_dir=RUN_DIR,\n",
        "    train_loss=train_loss,\n",
        "    train_acc=train_acc,\n",
        "    val_loss=val_loss,\n",
        "    val_acc=val_acc,\n",
        "    val_report=val_report,\n",
        "    test_loss=test_loss,\n",
        "    test_report=test_report,\n",
        "    train_max_samples=train_max_samples,\n",
        "    val_max_samples=val_max_samples,\n",
        "    test_max_samples=test_max_samples\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBXaRogcnMSE",
        "outputId": "1cef2aeb-a265-435b-aeff-cd5948bb90b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR â†’ Pred: 1 | Real: 0 | Path: /content/drive/Othercomputers/My Mac/Data/test/5028/spoof/538544.png\n"
          ]
        }
      ],
      "source": [
        "# Mostrar 10 predicciones incorrectas con su path\n",
        "for i, (img, label) in enumerate(test_dataset2):\n",
        "    pred = model(img.unsqueeze(0).to(device)).argmax(dim=1).item()\n",
        "    if pred != label:\n",
        "        print(f\"ERROR â†’ Pred: {pred} | Real: {label} | Path: {test_dataset2.image_paths[i]}\")\n",
        "        if i > 10: break\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMzPqtTdvmHPBdObZdflVLA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}