{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrorostagno/tesis/blob/main/Tesis_nuevo2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qavi6v5_xrjI"
      },
      "outputs": [],
      "source": [
        "# Usar A100, es el que mejor resultados me dio hasta ahora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5djEVQr1xWU6",
        "outputId": "82358bdb-d310-4ddb-ff4c-2836aeca7971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "550-DVSV3SD2",
        "outputId": "b6643878-b2b7-41e8-c778-d1939a1e8e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "files = os.listdir('/content/drive/Othercomputers/My Mac/Data/train/train/spoof/part_000')\n",
        "print(len(files))  # Si esto ya crashea, ya encontraste el cuello de botella\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLXmQkaSxz8D",
        "outputId": "65786290-d345-481f-b8a8-d54c3af7f979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Cach√©s en:     /content/drive/MyDrive/Tesis UTDT/cache\n",
            "üìÅ Resultados en: /content/drive/MyDrive/Tesis UTDT/experimentos/20250701-210846\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "dataset_path = '/content/drive/Othercomputers/My Mac/Data/train'\n",
        "\n",
        "# üóÇÔ∏è Carpetas base en Drive\n",
        "BASE_DIR = \"/content/drive/MyDrive/Tesis UTDT\"\n",
        "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
        "EXPERIMENTS_DIR = os.path.join(BASE_DIR, \"experimentos\")\n",
        "\n",
        "# ‚è±Ô∏è Nombre √∫nico del experimento basado en fecha y hora\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "RUN_DIR = os.path.join(EXPERIMENTS_DIR, timestamp)\n",
        "\n",
        "# üìÅ Crear carpetas si no existen\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Cach√©s en:     {CACHE_DIR}\")\n",
        "print(f\"üìÅ Resultados en: {RUN_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRN9PQq_0EoH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "class CelebASpoofDatasetFromCSV(Dataset):\n",
        "    def __init__(self, csv_path, transform=None, max_samples=None,\n",
        "                 shuffle=True, seed=42, balanced=False):\n",
        "        \"\"\"\n",
        "        Dataset desde CSV con opci√≥n de balanceo.\n",
        "\n",
        "        CSV esperado con columnas: 'path', 'label'\n",
        "        - label: 0 (spoof), 1 (live)\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        df = pd.read_csv(csv_path)\n",
        "        random.seed(seed)\n",
        "\n",
        "        if balanced:\n",
        "            spoof_df = df[df['label'] == 0]\n",
        "            live_df  = df[df['label'] == 1]\n",
        "            min_len = min(len(spoof_df), len(live_df))\n",
        "\n",
        "            spoof_df = spoof_df.sample(n=min_len, random_state=seed)\n",
        "            live_df  = live_df.sample(n=min_len, random_state=seed)\n",
        "            df = pd.concat([spoof_df, live_df], ignore_index=True)\n",
        "\n",
        "            print(f\"‚öñÔ∏è Dataset balanceado: {min_len} spoof + {min_len} live = {2 * min_len} total\")\n",
        "\n",
        "        if shuffle:\n",
        "            df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "        if max_samples:\n",
        "            df = df.head(max_samples)\n",
        "\n",
        "        self.image_paths = df['path'].tolist()\n",
        "        self.labels = df['label'].tolist()\n",
        "\n",
        "        print(f\"üìÑ CSV cargado: {csv_path}\")\n",
        "        print(f\"üìä Total im√°genes cargadas: {len(self.image_paths)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q5n9D1s01lK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# üìÅ Definici√≥n de rutas a cada partici√≥n del dataset (entrenamiento, validaci√≥n y test)\n",
        "train_data_dir = '/content/drive/Othercomputers/My Mac/Data/train/train'\n",
        "val_data_dir   = '/content/drive/Othercomputers/My Mac/Data/train/val'\n",
        "test_data_dir  = '/content/drive/Othercomputers/My Mac/Data/train/test'\n",
        "\n",
        "# üìê Dimensiones est√°ndar a las que se redimensionar√°n todas las im√°genes\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# ‚öôÔ∏è Tama√±o del batch para los DataLoaders\n",
        "batch_size = 32\n",
        "\n",
        "# üåÄ Transformaciones de preprocesamiento para cada conjunto de datos\n",
        "data_transforms = {\n",
        "    # 'train': transforms.Compose([\n",
        "    #     transforms.Resize((img_width, img_height)),              # Redimensiona la imagen a 224x224\n",
        "    #     transforms.RandomHorizontalFlip(),                       # Aplica flip horizontal aleatorio (augmentaci√≥n)\n",
        "    #     transforms.ToTensor(),                                   # Convierte PIL Image a tensor\n",
        "    #     transforms.Normalize([0.485, 0.456, 0.406],              # Normalizaci√≥n con media y desv√≠o est√°ndar de ImageNet\n",
        "    #                          [0.229, 0.224, 0.225])\n",
        "    # ]),\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((img_width + 32, img_height + 32)),    # Aumenta resoluci√≥n previa a crop\n",
        "        transforms.RandomResizedCrop((img_width, img_height), scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.02),\n",
        "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),\n",
        "        transforms.RandomApply([transforms.RandomRotation(10)], p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((img_width, img_height)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((img_width, img_height)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhXgn2Pn5wKp",
        "outputId": "acf0c1fb-7e70-4bf5-d610-9ef0adac2495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öñÔ∏è Dataset balanceado: 106825 spoof + 106825 live = 213650 total\n",
            "üìÑ CSV cargado: /content/drive/Othercomputers/My Mac/Data/train/train_image_list.csv\n",
            "üìä Total im√°genes cargadas: 100000\n",
            "‚öñÔ∏è Dataset balanceado: 23863 spoof + 23863 live = 47726 total\n",
            "üìÑ CSV cargado: /content/drive/Othercomputers/My Mac/Data/train/val_image_list.csv\n",
            "üìä Total im√°genes cargadas: 20000\n"
          ]
        }
      ],
      "source": [
        "# Cantidad m√°xima de muestras por partici√≥n\n",
        "train_max_samples = 100000\n",
        "val_max_samples   = 20000\n",
        "test_max_samples  = 5000\n",
        "\n",
        "# üì¶ Dataset de entrenamiento desde CSV\n",
        "train_dataset = CelebASpoofDatasetFromCSV(\n",
        "    csv_path='/content/drive/Othercomputers/My Mac/Data/train/train_image_list.csv',\n",
        "    transform=data_transforms['train'],\n",
        "    max_samples=train_max_samples,\n",
        "    balanced=True\n",
        ")\n",
        "\n",
        "# üì¶ Dataset de validaci√≥n desde CSV\n",
        "val_dataset = CelebASpoofDatasetFromCSV(\n",
        "    csv_path='/content/drive/Othercomputers/My Mac/Data/train/val_image_list.csv',\n",
        "    transform=data_transforms['val'],\n",
        "    max_samples=val_max_samples,\n",
        "    balanced=True\n",
        ")\n",
        "\n",
        "# üì¶ Dataset de test desde CSV\n",
        "# test_dataset = CelebASpoofDatasetFromCSV(\n",
        "#     csv_path='/content/drive/Othercomputers/My Mac/Data/train/test_image_list.csv',\n",
        "#     transform=data_transforms['test'],\n",
        "#     max_samples=test_max_samples\n",
        "# )\n",
        "\n",
        "# üîÑ DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=10,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=10,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# test_loader = DataLoader(\n",
        "#     test_dataset,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=False,\n",
        "#     num_workers=8,\n",
        "#     pin_memory=True\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFdseMBWeQ3Q",
        "outputId": "e14e166f-b005-4e34-ec42-985414bd05a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distribuci√≥n de clases en entrenamiento: {np.int64(0): np.int64(49785), np.int64(1): np.int64(50215)}\n"
          ]
        }
      ],
      "source": [
        "labels = np.array(train_dataset.labels)\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print(f\"Distribuci√≥n de clases en entrenamiento: {dict(zip(unique, counts))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPA07Eq43g4p",
        "outputId": "5a5c4012-c34b-4eb5-e9f7-3df531a9cbf2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ 1) MODELO  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from torchvision import models                     # ‚Üê ya lo usabas\n",
        "\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "num_ftrs = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_ftrs, 2)       # 2 clases\n",
        "\n",
        "# (opcional) descongela todo el backbone para fine-tuning completo\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62VUKKYL6vcy"
      },
      "outputs": [],
      "source": [
        "import os, json, torch, sklearn.metrics as metrics\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import amp                      # üëà  nueva forma\n",
        "from collections import OrderedDict\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "#  1) FUNCI√ìN DE P√âRDIDA ‚Äì FOCAL LOSS\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=(0.25, 0.75), gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = torch.tensor(alpha)\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        # mueve alpha al device del tensor de logits\n",
        "        if self.alpha.device != logits.device:\n",
        "            self.alpha = self.alpha.to(logits.device)\n",
        "\n",
        "        ce_loss = self.ce(logits, targets)\n",
        "        pt      = torch.exp(-ce_loss)\n",
        "        at      = self.alpha.gather(0, targets)\n",
        "        loss    = at * (1 - pt) ** self.gamma * ce_loss\n",
        "        return loss.mean() if self.reduction == 'mean' else loss\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 2) BUCLE DE ENTRENAMIENTO (precisi√≥n mixta moderna)\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def train_epoch(model, loader, optimizer, criterion, device, scaler):\n",
        "    model.train()\n",
        "    loss_sum, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with amp.autocast(device_type='cuda'):\n",
        "            logits = model(x)\n",
        "            loss   = criterion(logits, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        correct  += (logits.argmax(1) == y).sum().item()\n",
        "        total    += x.size(0)\n",
        "\n",
        "    return loss_sum / total, correct / total\n",
        "\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    loss_sum, correct, total = 0.0, 0, 0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            with amp.autocast(device_type='cuda'):\n",
        "                logits = model(x)\n",
        "                loss   = criterion(logits, y)\n",
        "\n",
        "            loss_sum += loss.item() * x.size(0)\n",
        "            correct  += (logits.argmax(1) == y).sum().item()\n",
        "            total    += x.size(0)\n",
        "\n",
        "            y_true.extend(y.cpu().numpy())\n",
        "            y_pred.extend(logits.argmax(1).cpu().numpy())\n",
        "\n",
        "    report = metrics.classification_report(\n",
        "        y_true, y_pred, output_dict=True, zero_division=0\n",
        "    )\n",
        "    return loss_sum / total, correct / total, report\n",
        "\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# 3) FUNCI√ìN PRINCIPAL DE ENTRENAMIENTO\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def train_model(\n",
        "        model, train_loader, val_loader,\n",
        "        criterion, optimizer, scheduler, device,\n",
        "        num_epochs=20, patience=6, checkpoint_interval=1, log_dir=None):\n",
        "\n",
        "    if log_dir is None:\n",
        "        log_dir = os.path.join(RUN_DIR, \"tensorboard\")\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    writer = SummaryWriter(log_dir)\n",
        "\n",
        "    # guardar config\n",
        "    json.dump(OrderedDict(\n",
        "        num_epochs=num_epochs,\n",
        "        patience=patience,\n",
        "        batch_size=train_loader.batch_size,\n",
        "        optimizer=type(optimizer).__name__,\n",
        "        lr_start=optimizer.param_groups[0][\"lr\"]\n",
        "    ), open(os.path.join(RUN_DIR, \"config.json\"), \"w\"), indent=2)\n",
        "\n",
        "    scaler          = amp.GradScaler()\n",
        "    best_val_loss   = float('inf')\n",
        "    wait            = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs}\\n\" + \"-\" * 20)\n",
        "\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer,\n",
        "                                      criterion, device, scaler)\n",
        "        val_loss, val_acc, report = validate_epoch(\n",
        "            model, val_loader, criterion, device)\n",
        "\n",
        "        scheduler.step()   # CosineAnnealingLR no necesita m√©tricas\n",
        "\n",
        "        print(f\"LR: {scheduler.get_last_lr()[0]:.6f}  \"\n",
        "              f\"TrainLoss: {tr_loss:.4f}  ValLoss: {val_loss:.4f}\")\n",
        "        print(f\"Val Acc: {val_acc:.4f}  F1_spoof: {report['0']['f1-score']:.3f}\")\n",
        "\n",
        "        # logs tensorboard\n",
        "        writer.add_scalars(\"Loss\", {\"train\": tr_loss, \"val\": val_loss}, epoch)\n",
        "        writer.add_scalars(\"Acc\",  {\"train\": tr_acc , \"val\": val_acc }, epoch)\n",
        "\n",
        "        # early-stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss, wait = val_loss, 0\n",
        "            torch.save(model.state_dict(), os.path.join(RUN_DIR, \"best_model.pth\"))\n",
        "            print(\"‚úÖ  Nuevo mejor modelo guardado\")\n",
        "        else:\n",
        "            wait += 1\n",
        "            print(f\"‚è≥  Sin mejora ({wait}/{patience})\")\n",
        "            if wait >= patience:\n",
        "                print(\"üõë  Early stopping\")\n",
        "                break\n",
        "\n",
        "        # checkpoint peri√≥dico\n",
        "        if epoch % checkpoint_interval == 0:\n",
        "            torch.save({\"epoch\": epoch,\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"optim\": optimizer.state_dict()},\n",
        "                       os.path.join(RUN_DIR, f\"ckpt_{epoch:02d}.pth\"))\n",
        "    writer.close()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNr4aLfGUprl"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model  = model.to(device)                          # tu MobileNetV2 ya modificado\n",
        "\n",
        "criterion  = FocalLoss()                           # ‚Üê nueva loss\n",
        "optimizer  = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "scheduler  = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NjeBDJpTUQQ",
        "outputId": "d59e1fd8-f981-44dc-960d-7cc3c995c5f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "--------------------\n",
            "LR: 0.000497  TrainLoss: 0.0099  ValLoss: 0.0027\n",
            "Val Acc: 0.9922  F1_spoof: 0.992\n",
            "‚úÖ  Nuevo mejor modelo guardado\n",
            "\n",
            "Epoch 2/10\n",
            "--------------------\n",
            "LR: 0.000488  TrainLoss: 0.0064  ValLoss: 0.0021\n",
            "Val Acc: 0.9936  F1_spoof: 0.994\n",
            "‚úÖ  Nuevo mejor modelo guardado\n",
            "\n",
            "Epoch 3/10\n",
            "--------------------\n",
            "LR: 0.000473  TrainLoss: 0.0052  ValLoss: 0.0026\n",
            "Val Acc: 0.9914  F1_spoof: 0.991\n",
            "‚è≥  Sin mejora (1/6)\n",
            "\n",
            "Epoch 4/10\n",
            "--------------------\n",
            "LR: 0.000452  TrainLoss: 0.0042  ValLoss: 0.0030\n",
            "Val Acc: 0.9939  F1_spoof: 0.994\n",
            "‚è≥  Sin mejora (2/6)\n",
            "\n",
            "Epoch 5/10\n",
            "--------------------\n",
            "LR: 0.000427  TrainLoss: 0.0038  ValLoss: 0.0018\n",
            "Val Acc: 0.9947  F1_spoof: 0.995\n",
            "‚úÖ  Nuevo mejor modelo guardado\n",
            "\n",
            "Epoch 6/10\n",
            "--------------------\n",
            "LR: 0.000397  TrainLoss: 0.0034  ValLoss: 0.0012\n",
            "Val Acc: 0.9969  F1_spoof: 0.997\n",
            "‚úÖ  Nuevo mejor modelo guardado\n",
            "\n",
            "Epoch 7/10\n",
            "--------------------\n",
            "LR: 0.000363  TrainLoss: 0.0029  ValLoss: 0.0020\n",
            "Val Acc: 0.9936  F1_spoof: 0.994\n",
            "‚è≥  Sin mejora (1/6)\n",
            "\n",
            "Epoch 8/10\n",
            "--------------------\n",
            "LR: 0.000327  TrainLoss: 0.0025  ValLoss: 0.0012\n",
            "Val Acc: 0.9966  F1_spoof: 0.997\n",
            "‚è≥  Sin mejora (2/6)\n",
            "\n",
            "Epoch 9/10\n",
            "--------------------\n",
            "LR: 0.000289  TrainLoss: 0.0021  ValLoss: 0.0014\n",
            "Val Acc: 0.9964  F1_spoof: 0.996\n",
            "‚è≥  Sin mejora (3/6)\n",
            "\n",
            "Epoch 10/10\n",
            "--------------------\n",
            "LR: 0.000250  TrainLoss: 0.0019  ValLoss: 0.0015\n",
            "Val Acc: 0.9956  F1_spoof: 0.996\n",
            "‚è≥  Sin mejora (4/6)\n"
          ]
        }
      ],
      "source": [
        "# continua del bloque anterior, pero lo pongo aca para tenerlo separado\n",
        "# üß™ Ejecuci√≥n del entrenamiento (se asume que todo est√° definido)\n",
        "trained_model = train_model(\n",
        "        model, train_loader, val_loader,\n",
        "        criterion, optimizer, scheduler, device,\n",
        "        num_epochs=10, patience=6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH9KuuHz72Uf",
        "outputId": "e8edc794-7530-4ff0-c896-f378252a612b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mejor modelo cargado desde RUN_DIR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# üß† Cargar el mejor modelo desde el directorio de la corrida actual\n",
        "best_model = models.mobilenet_v2(pretrained=False)\n",
        "num_ftrs = best_model.classifier[1].in_features\n",
        "best_model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
        "best_model.load_state_dict(torch.load(os.path.join(RUN_DIR, 'best_model.pth')))\n",
        "best_model = best_model.to(device)\n",
        "print(\"‚úÖ Mejor modelo cargado desde RUN_DIR\")\n",
        "\n",
        "\n",
        "\n",
        "def test_model(model, test_loader, criterion, device, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    test_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Usar autocast para precisi√≥n mixta en la inferencia\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            total_samples += inputs.size(0)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = test_loss / total_samples\n",
        "    print(f'Test Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Convertir a arrays de numpy\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calcular matriz de confusi√≥n y reporte en formato dict\n",
        "    cm = metrics.confusion_matrix(all_labels, all_preds)\n",
        "    report = metrics.classification_report(all_labels, all_preds, target_names=[\"spoof\", \"live\"], zero_division=0)\n",
        "    report_dict = metrics.classification_report(all_labels, all_preds, target_names=[\"spoof\", \"live\"], output_dict=True, zero_division=0)\n",
        "\n",
        "    # Guardar m√©tricas como JSON\n",
        "    with open(os.path.join(output_dir, 'classification_report.json'), 'w') as f:\n",
        "        json.dump(report, f, indent=4)\n",
        "    print(\"üìÑ Reporte de clasificaci√≥n guardado en JSON.\")\n",
        "\n",
        "    print(\"Matriz de Confusi√≥n:\")\n",
        "    print(cm)\n",
        "    print(\"Reporte de Clasificaci√≥n:\")\n",
        "    print(report)\n",
        "\n",
        "    # Guardar matriz de confusi√≥n como imagen PNG\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"spoof\", \"live\"], yticklabels=[\"spoof\", \"live\"])\n",
        "    plt.xlabel(\"Predicci√≥n\")\n",
        "    plt.ylabel(\"Real\")\n",
        "    plt.title(\"Matriz de Confusi√≥n\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "    print(\"üñºÔ∏è Matriz de confusi√≥n guardada como imagen.\")\n",
        "\n",
        "    return avg_loss, report_dict, cm\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGz2agcmUXdn",
        "outputId": "7869561d-6cf1-4b67-92cb-39b9bf1aec44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ CSV cargado: /content/drive/Othercomputers/My Mac/Data/train/test_image_list.csv\n",
            "üìä Total im√°genes cargadas: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-40-328312363.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0022\n",
            "üìÑ Reporte de clasificaci√≥n guardado en JSON.\n",
            "Matriz de Confusi√≥n:\n",
            "[[7259   96]\n",
            " [  11 2634]]\n",
            "Reporte de Clasificaci√≥n:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       spoof       1.00      0.99      0.99      7355\n",
            "        live       0.96      1.00      0.98      2645\n",
            "\n",
            "    accuracy                           0.99     10000\n",
            "   macro avg       0.98      0.99      0.99     10000\n",
            "weighted avg       0.99      0.99      0.99     10000\n",
            "\n",
            "üñºÔ∏è Matriz de confusi√≥n guardada como imagen.\n"
          ]
        }
      ],
      "source": [
        "# Continuacion del bloque anterior. Testeo del modelo en otro dataset\n",
        "test_data_dir2 = '/content/drive/Othercomputers/My Mac/Data/test'\n",
        "\n",
        "\n",
        "# üì¶ Dataset de test desde CSV\n",
        "test_dataset = CelebASpoofDatasetFromCSV(\n",
        "    csv_path='/content/drive/Othercomputers/My Mac/Data/train/test_image_list.csv',\n",
        "    transform=data_transforms['test'],\n",
        "    max_samples=10000\n",
        ")\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "test_loader2 = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "\n",
        "# Ejemplo de uso:\n",
        "# test_loss, test_report, test_cm = test_model(model, test_loader2, criterion, device)\n",
        "test_loss, test_report, test_cm = test_model(best_model, test_loader2, criterion, device, output_dir=RUN_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDcmFDyOvqdc",
        "outputId": "1f2ce79f-ceec-45f6-da78-d11f5bee8861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ CSV cargado: /content/drive/Othercomputers/My Mac/Data/train/test2_image_list.csv\n",
            "üìä Total im√°genes cargadas: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-40-328312363.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1780\n",
            "üìÑ Reporte de clasificaci√≥n guardado en JSON.\n",
            "Matriz de Confusi√≥n:\n",
            "[[4081 2991]\n",
            " [   3 2925]]\n",
            "Reporte de Clasificaci√≥n:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       spoof       1.00      0.58      0.73      7072\n",
            "        live       0.49      1.00      0.66      2928\n",
            "\n",
            "    accuracy                           0.70     10000\n",
            "   macro avg       0.75      0.79      0.70     10000\n",
            "weighted avg       0.85      0.70      0.71     10000\n",
            "\n",
            "üñºÔ∏è Matriz de confusi√≥n guardada como imagen.\n"
          ]
        }
      ],
      "source": [
        "# Continuacion del bloque anterior. Testeo del modelo en otro dataset\n",
        "test_data_dir2 = '/content/drive/Othercomputers/My Mac/Data/test'\n",
        "\n",
        "\n",
        "# üì¶ Dataset de test desde CSV\n",
        "test_dataset2 = CelebASpoofDatasetFromCSV(\n",
        "    csv_path='/content/drive/Othercomputers/My Mac/Data/train/test2_image_list.csv',\n",
        "    transform=data_transforms['test'],\n",
        "    max_samples=10000\n",
        ")\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "\n",
        "# Ejemplo de uso:\n",
        "# test_loss, test_report, test_cm = test_model(model, test_loader2, criterion, device)\n",
        "test_loss, test_report, test_cm = test_model(best_model, test_loader2, criterion, device, output_dir=RUN_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW8B2HBXX5-M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def log_experiment_summary(run_dir,\n",
        "                           train_loss, train_acc,\n",
        "                           val_loss, val_acc, val_report,\n",
        "                           test_loss, test_report,\n",
        "                           train_max_samples, val_max_samples, test_max_samples):\n",
        "    summary_path = os.path.join(EXPERIMENTS_DIR, \"all_runs.csv\")\n",
        "    run_name = os.path.basename(run_dir)\n",
        "\n",
        "    # Utilidad segura para extraer m√©tricas, aunque una clase no est√© presente\n",
        "    def get_metric(report, class_name, metric):\n",
        "        return round(report.get(class_name, {}).get(metric, 0.0), 4)\n",
        "\n",
        "    summary_data = {\n",
        "        \"run_name\": run_name,\n",
        "\n",
        "        # M√©tricas de entrenamiento\n",
        "        \"train_loss\": round(train_loss, 4),\n",
        "        \"train_acc\": round(train_acc.item(), 4),\n",
        "\n",
        "        # M√©tricas de validaci√≥n\n",
        "        \"val_loss\": round(val_loss, 4),\n",
        "        \"val_acc\": round(val_acc.item(), 4),\n",
        "        \"f1_macro_val\": get_metric(val_report, \"macro avg\", \"f1-score\"),\n",
        "        \"precision_spoof_val\": get_metric(val_report, \"0\", \"precision\"),\n",
        "        \"recall_spoof_val\": get_metric(val_report, \"0\", \"recall\"),\n",
        "        \"f1_spoof_val\": get_metric(val_report, \"0\", \"f1-score\"),\n",
        "        \"precision_live_val\": get_metric(val_report, \"1\", \"precision\"),\n",
        "        \"recall_live_val\": get_metric(val_report, \"1\", \"recall\"),\n",
        "        \"f1_live_val\": get_metric(val_report, \"1\", \"f1-score\"),\n",
        "\n",
        "        # M√©tricas de test final\n",
        "        \"test_loss\": round(test_loss, 4),\n",
        "        \"f1_macro_test\": get_metric(test_report, \"macro avg\", \"f1-score\"),\n",
        "        \"precision_spoof_test\": get_metric(test_report, \"spoof\", \"precision\"),\n",
        "        \"recall_spoof_test\": get_metric(test_report, \"spoof\", \"recall\"),\n",
        "        \"f1_spoof_test\": get_metric(test_report, \"spoof\", \"f1-score\"),\n",
        "        \"precision_live_test\": get_metric(test_report, \"live\", \"precision\"),\n",
        "        \"recall_live_test\": get_metric(test_report, \"live\", \"recall\"),\n",
        "        \"f1_live_test\": get_metric(test_report, \"live\", \"f1-score\"),\n",
        "\n",
        "        # Info de dataset\n",
        "        \"train_max_samples\": train_max_samples,\n",
        "        \"val_max_samples\": val_max_samples,\n",
        "        \"test_max_samples\": test_max_samples\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', header=False, index=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"üìù Resumen de la corrida guardado en {summary_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAsTob5fZ_iW",
        "outputId": "b93c053e-b5cd-4f30-b8a6-9c30f7f4579d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'spoof': {'precision': 0.9924892703862661,\n",
              "  'recall': 0.3474178403755869,\n",
              "  'f1-score': 0.514675198219502,\n",
              "  'support': 5325.0},\n",
              " 'live': {'precision': 0.5728859390363815,\n",
              "  'recall': 0.9970053475935828,\n",
              "  'f1-score': 0.7276559206931543,\n",
              "  'support': 4675.0},\n",
              " 'accuracy': 0.6511,\n",
              " 'macro avg': {'precision': 0.7826876047113238,\n",
              "  'recall': 0.6722115939845849,\n",
              "  'f1-score': 0.6211655594563281,\n",
              "  'support': 10000.0},\n",
              " 'weighted avg': {'precision': 0.796324712980195,\n",
              "  'recall': 0.6511,\n",
              "  'f1-score': 0.6142436859759345,\n",
              "  'support': 10000.0}}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE0EMB-aYERA",
        "outputId": "03344fe2-9d90-4424-b212-0779525a25ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Resumen de la corrida guardado en /content/drive/MyDrive/Tesis UTDT/experimentos/all_runs.csv\n"
          ]
        }
      ],
      "source": [
        "log_experiment_summary(\n",
        "    run_dir=RUN_DIR,\n",
        "    train_loss=train_loss,\n",
        "    train_acc=train_acc,\n",
        "    val_loss=val_loss,\n",
        "    val_acc=val_acc,\n",
        "    val_report=val_report,\n",
        "    test_loss=test_loss,\n",
        "    test_report=test_report,\n",
        "    train_max_samples=train_max_samples,\n",
        "    val_max_samples=val_max_samples,\n",
        "    test_max_samples=test_max_samples\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBXaRogcnMSE",
        "outputId": "1cef2aeb-a265-435b-aeff-cd5948bb90b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR ‚Üí Pred: 1 | Real: 0 | Path: /content/drive/Othercomputers/My Mac/Data/test/5028/spoof/538544.png\n"
          ]
        }
      ],
      "source": [
        "# Mostrar 10 predicciones incorrectas con su path\n",
        "for i, (img, label) in enumerate(test_dataset2):\n",
        "    pred = model(img.unsqueeze(0).to(device)).argmax(dim=1).item()\n",
        "    if pred != label:\n",
        "        print(f\"ERROR ‚Üí Pred: {pred} | Real: {label} | Path: {test_dataset2.image_paths[i]}\")\n",
        "        if i > 10: break\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMzPqtTdvmHPBdObZdflVLA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}