{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP3MpQN93+zkSVxTV6y5Zlq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrorostagno/tesis/blob/main/Tesis_nuevo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usar A100, es el que mejor resultados me dio hasta ahora"
      ],
      "metadata": {
        "id": "Qavi6v5_xrjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5djEVQr1xWU6",
        "outputId": "2f984f44-772b-41a5-efd0-5e0b871bc805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "dataset_path = '/content/drive/Othercomputers/My Mac/Data/train'\n",
        "\n",
        "# üóÇÔ∏è Carpetas base en Drive\n",
        "BASE_DIR = \"/content/drive/MyDrive/Tesis UTDT\"\n",
        "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
        "EXPERIMENTS_DIR = os.path.join(BASE_DIR, \"experimentos\")\n",
        "\n",
        "# ‚è±Ô∏è Nombre √∫nico del experimento basado en fecha y hora\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "RUN_DIR = os.path.join(EXPERIMENTS_DIR, timestamp)\n",
        "\n",
        "# üìÅ Crear carpetas si no existen\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Cach√©s en:     {CACHE_DIR}\")\n",
        "print(f\"üìÅ Resultados en: {RUN_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLXmQkaSxz8D",
        "outputId": "d42f2190-82ec-4b9c-c93d-a50d4fff51a7"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Cach√©s en:     /content/drive/MyDrive/Tesis UTDT/cache\n",
            "üìÅ Resultados en: /content/drive/MyDrive/Tesis UTDT/experimentos/20250622-221825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # üì¶ Clase personalizada para cargar el dataset CelebA-Spoof (u otro similar estructurado por carpetas)\n",
        "\n",
        "# class CelebASpoofDataset(Dataset):\n",
        "#     def __init__(self, root_dir, transform=None, cache_file=None, max_samples=None):\n",
        "#         \"\"\"\n",
        "#         Inicializa el dataset.\n",
        "\n",
        "#         Par√°metros:\n",
        "#         - root_dir (str): Ruta ra√≠z del dataset con estructura jer√°rquica (sujetos > spoof/live > im√°genes).\n",
        "#         - transform (callable, opcional): Transformaciones a aplicar a cada imagen.\n",
        "#         - cache_file (str, opcional): Ruta para guardar o cargar el cache de rutas e √≠ndices.\n",
        "#         - max_samples (int, opcional): L√≠mite m√°ximo de muestras a cargar (√∫til para pruebas r√°pidas).\n",
        "#         \"\"\"\n",
        "#         self.root_dir = root_dir\n",
        "#         self.transform = transform\n",
        "#         self.image_paths = []\n",
        "#         self.labels = []\n",
        "\n",
        "#         if cache_file is not None and os.path.exists(cache_file):\n",
        "#             print(\"Cargando dataset desde cache...\")\n",
        "#             with open(cache_file, 'rb') as f:\n",
        "#                 self.image_paths, self.labels = pickle.load(f)\n",
        "#         else:\n",
        "#             print(\"Generando la lista de im√°genes usando os.scandir...\")\n",
        "#             # Variable para determinar si ya se alcanz√≥ el m√°ximo de muestras\n",
        "#             max_reached = False\n",
        "#             with os.scandir(root_dir) as subjects:\n",
        "#                 for subject in subjects:\n",
        "#                     if max_samples is not None and len(self.image_paths) >= max_samples:\n",
        "#                         max_reached = True\n",
        "#                         break\n",
        "#                     if subject.is_dir():\n",
        "#                         subject_path = subject.path\n",
        "#                         with os.scandir(subject_path) as type_entries:\n",
        "#                             for entry in type_entries:\n",
        "#                                 if max_samples is not None and len(self.image_paths) >= max_samples:\n",
        "#                                     max_reached = True\n",
        "#                                     break\n",
        "#                                 if entry.is_dir():\n",
        "#                                     image_type = entry.name  # 'live' o 'spoof'\n",
        "#                                     with os.scandir(entry.path) as files:\n",
        "#                                         for file_entry in files:\n",
        "#                                             if file_entry.is_file() and file_entry.name.lower().endswith(('.jpg', '.png')):\n",
        "#                                                 self.image_paths.append(file_entry.path)\n",
        "#                                                 self.labels.append(0 if image_type == 'spoof' else 1)\n",
        "#                                                 if max_samples is not None and len(self.image_paths) >= max_samples:\n",
        "#                                                     max_reached = True\n",
        "#                                                     break\n",
        "#                                         if max_samples is not None and len(self.image_paths) >= max_samples:\n",
        "#                                             break\n",
        "#                             if max_samples is not None and len(self.image_paths) >= max_samples:\n",
        "#                                 break\n",
        "#                     if max_reached:\n",
        "#                         break\n",
        "#             # Guardar en cache si se especifica\n",
        "#             if cache_file is not None:\n",
        "#                 with open(cache_file, 'wb') as f:\n",
        "#                     pickle.dump((self.image_paths, self.labels), f)\n",
        "#                 print(\"Cache guardado en\", cache_file)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         \"\"\"Devuelve la cantidad total de muestras en el dataset.\"\"\"\n",
        "#         return len(self.image_paths)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         \"\"\"\n",
        "#         Devuelve una imagen y su etiqueta correspondiente.\n",
        "\n",
        "#         - Abre la imagen en formato RGB.\n",
        "#         - Aplica transformaciones si fueron especificadas.\n",
        "#         - Devuelve un tensor y un entero (0=spoof, 1=live).\n",
        "#         \"\"\"\n",
        "#         image_path = self.image_paths[idx]\n",
        "#         image = Image.open(image_path).convert('RGB')\n",
        "#         label = self.labels[idx]\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "#         return image, label\n",
        "\n",
        "class CelebASpoofDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, cache_file=None, max_samples=None, balance=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        if cache_file is not None and os.path.exists(cache_file):\n",
        "            print(\"Cargando dataset desde cache...\")\n",
        "            with open(cache_file, 'rb') as f:\n",
        "                self.image_paths, self.labels = pickle.load(f)\n",
        "        else:\n",
        "            print(\"Generando la lista de im√°genes usando os.scandir...\")\n",
        "            max_reached = False\n",
        "            with os.scandir(root_dir) as subjects:\n",
        "                for subject in subjects:\n",
        "                    if max_samples is not None and len(self.image_paths) >= max_samples:\n",
        "                        max_reached = True\n",
        "                        break\n",
        "                    if subject.is_dir():\n",
        "                        subject_path = subject.path\n",
        "                        with os.scandir(subject_path) as type_entries:\n",
        "                            for entry in type_entries:\n",
        "                                if max_samples is not None and len(self.image_paths) >= max_samples:\n",
        "                                    max_reached = True\n",
        "                                    break\n",
        "                                if entry.is_dir():\n",
        "                                    image_type = entry.name\n",
        "                                    with os.scandir(entry.path) as files:\n",
        "                                        for file_entry in files:\n",
        "                                            if file_entry.is_file() and file_entry.name.lower().endswith(('.jpg', '.png')):\n",
        "                                                self.image_paths.append(file_entry.path)\n",
        "                                                self.labels.append(0 if image_type == 'spoof' else 1)\n",
        "                                                if max_samples is not None and len(self.image_paths) >= max_samples:\n",
        "                                                    max_reached = True\n",
        "                                                    break\n",
        "                                        if max_reached:\n",
        "                                            break\n",
        "                            if max_reached:\n",
        "                                break\n",
        "                    if max_reached:\n",
        "                        break\n",
        "            if cache_file is not None:\n",
        "                with open(cache_file, 'wb') as f:\n",
        "                    pickle.dump((self.image_paths, self.labels), f)\n",
        "                print(\"Cache guardado en\", cache_file)\n",
        "\n",
        "        # ‚öñÔ∏è Balanceo posterior (solo si balance=True)\n",
        "        if balance:\n",
        "            print(\"‚öñÔ∏è Aplicando balance de clases...\")\n",
        "            spoof_samples = [(p, l) for p, l in zip(self.image_paths, self.labels) if l == 0]\n",
        "            live_samples  = [(p, l) for p, l in zip(self.image_paths, self.labels) if l == 1]\n",
        "            min_len = min(len(spoof_samples), len(live_samples))\n",
        "\n",
        "            # Cortamos la clase mayoritaria\n",
        "            spoof_samples = spoof_samples[:min_len]\n",
        "            live_samples = live_samples[:min_len]\n",
        "\n",
        "            combined = spoof_samples + live_samples\n",
        "            random.shuffle(combined)\n",
        "            self.image_paths, self.labels = zip(*combined)\n",
        "            self.image_paths = list(self.image_paths)\n",
        "            self.labels = list(self.labels)\n",
        "\n",
        "            print(f\"üìä Dataset balanceado: {min_len} spoof + {min_len} live = {2 * min_len} total\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GRN9PQq_0EoH"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# üìÅ Definici√≥n de rutas a cada partici√≥n del dataset (entrenamiento, validaci√≥n y test)\n",
        "train_data_dir = '/content/drive/Othercomputers/My Mac/Data/train/train'\n",
        "val_data_dir   = '/content/drive/Othercomputers/My Mac/Data/train/val'\n",
        "test_data_dir  = '/content/drive/Othercomputers/My Mac/Data/train/test'\n",
        "\n",
        "# üìê Dimensiones est√°ndar a las que se redimensionar√°n todas las im√°genes\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# ‚öôÔ∏è Tama√±o del batch para los DataLoaders\n",
        "batch_size = 128\n",
        "\n",
        "# üåÄ Transformaciones de preprocesamiento para cada conjunto de datos\n",
        "data_transforms = {\n",
        "    # 'train': transforms.Compose([\n",
        "    #     transforms.Resize((img_width, img_height)),              # Redimensiona la imagen a 224x224\n",
        "    #     transforms.RandomHorizontalFlip(),                       # Aplica flip horizontal aleatorio (augmentaci√≥n)\n",
        "    #     transforms.ToTensor(),                                   # Convierte PIL Image a tensor\n",
        "    #     transforms.Normalize([0.485, 0.456, 0.406],              # Normalizaci√≥n con media y desv√≠o est√°ndar de ImageNet\n",
        "    #                          [0.229, 0.224, 0.225])\n",
        "    # ]),\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((img_width + 32, img_height + 32)),    # Aumenta resoluci√≥n previa a crop\n",
        "        transforms.RandomResizedCrop((img_width, img_height), scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.02),\n",
        "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),\n",
        "        transforms.RandomApply([transforms.RandomRotation(10)], p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((img_width, img_height)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((img_width, img_height)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n"
      ],
      "metadata": {
        "id": "9q5n9D1s01lK"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "\n",
        "¬øPor qu√© se redimensiona a (224, 224)?\n",
        "\n",
        "Motivo principal: muchos modelos preentrenados (como MobileNetV2, ResNet, VGG) fueron entrenados originalmente con im√°genes de 224x224 p√≠xeles.\n",
        "Usar ese tama√±o:\n",
        "Permite reutilizar pesos preentrenados (transfer learning) sin modificar la arquitectura.\n",
        "Asegura una entrada consistente para la red (todas las im√°genes con el mismo tama√±o).\n",
        "Adem√°s, 224√ó224 es un buen balance entre:\n",
        "Calidad visual suficiente para tareas de clasificaci√≥n.\n",
        "Eficiencia computacional (no tan pesado como 512x512, por ejemplo).\n",
        "\n",
        "\n",
        "\n",
        "¬øQu√© significa batch_size = 32 y por qu√© ese n√∫mero?\n",
        "\n",
        "El batch size es la cantidad de im√°genes que se procesan simult√°neamente en una pasada (forward + backward) durante el entrenamiento.\n",
        "Tama√±o com√∫n: 8, 16, 32, 64, etc.\n",
        "Usar 32 implica:\n",
        "Buena estabilidad num√©rica del gradiente.\n",
        "Razonable uso de memoria en GPU (ni muy chico ni muy grande).\n",
        "En general:\n",
        "Batches chicos (8-16): m√°s precisos pero lentos.\n",
        "Batches grandes (64-128+): m√°s r√°pidos pero requieren m√°s memoria y pueden converger peor.\n",
        "Batch = 32 es una elecci√≥n segura, ampliamente utilizada, y probablemente compatible con el uso de GPU en Colab.\n",
        "\n",
        "\n",
        "\n",
        "Normalizacion\n",
        "\n",
        "Se utiliza la normalizaci√≥n est√°ndar de ImageNet (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) porque el modelo base empleado fue preentrenado sobre este dataset. Esta normalizaci√≥n asegura que las im√°genes de entrada tengan una distribuci√≥n similar a la vista durante el preentrenamiento, lo cual es esencial para transfer learning efectivo.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "5TU6nbZZ1HhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad m√°xima de muestras por partici√≥n (ajustable)\n",
        "train_max_samples = 50000\n",
        "val_max_samples = 10000\n",
        "test_max_samples = 5000\n",
        "\n",
        "# üì¶ Dataset de entrenamiento con cache y transformaci√≥n\n",
        "train_dataset = CelebASpoofDataset(\n",
        "    root_dir=train_data_dir,\n",
        "    transform=data_transforms['train'],\n",
        "    cache_file=os.path.join(CACHE_DIR, \"train_dataset_cache_1.pkl\"),\n",
        "    max_samples=train_max_samples,\n",
        "    balance=True\n",
        "    # max_subjects=300\n",
        ")\n",
        "\n",
        "# üì¶ Dataset de validaci√≥n\n",
        "val_dataset = CelebASpoofDataset(\n",
        "    root_dir=val_data_dir,\n",
        "    transform=data_transforms['val'],\n",
        "    cache_file=os.path.join(CACHE_DIR, \"val_dataset_cache_1.pkl\"),\n",
        "    max_samples=val_max_samples,\n",
        "    # balance=True\n",
        "    # max_subjects=300\n",
        ")\n",
        "\n",
        "# üì¶ Dataset de test\n",
        "# test_dataset = CelebASpoofDataset(\n",
        "#     root_dir=test_data_dir,\n",
        "#     transform=data_transforms['test'],\n",
        "#     cache_file=os.path.join(CACHE_DIR, \"test_dataset_cache.pkl\"),\n",
        "#     max_samples=test_max_samples\n",
        "#     # max_subjects=300\n",
        "# )\n",
        "\n",
        "# üîÑ DataLoaders para alimentar los datos al modelo en mini-batches\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,     # Cantidad de muestras por batch (definido previamente como 32)\n",
        "    shuffle=True,              # Mezcla los datos en cada √©poca (importante para entrenamiento)\n",
        "    num_workers=8,             # Procesos paralelos para cargar datos (ajustar seg√∫n GPU/Colab)\n",
        "    pin_memory=True            # Optimiza transferencia a GPU (si se usa CUDA)\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,             # No se mezcla para evaluaci√≥n consistente\n",
        "    num_workers=8,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# test_loader = DataLoader(\n",
        "#     test_dataset,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=False,\n",
        "#     num_workers=8,\n",
        "#     pin_memory=True\n",
        "# )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhXgn2Pn5wKp",
        "outputId": "ebcc7f5c-34a6-4227-b767-7ec18442f796"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando dataset desde cache...\n",
            "‚öñÔ∏è Aplicando balance de clases...\n",
            "üìä Dataset balanceado: 17123 spoof + 17123 live = 34246 total\n",
            "Generando la lista de im√°genes usando os.scandir...\n",
            "Cache guardado en /content/drive/MyDrive/Tesis UTDT/cache/val_dataset_cache_1.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# üß† Carga de modelo preentrenado\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# üîß Modificaci√≥n de la √∫ltima capa para clasificaci√≥n binaria\n",
        "# Extraemos el n√∫mero de entradas de la capa final\n",
        "num_ftrs = model.classifier[1].in_features\n",
        "\n",
        "# Reemplazamos la √∫ltima capa por una nueva totalmente conectada con 2 salidas (spoof / live)\n",
        "model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "\n",
        "# ‚öôÔ∏è Optimizador\n",
        "# Adam es un optimizador eficiente y ampliamente utilizado\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005) #Cambie el learning rate de 0.001 a 0.0005\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPA07Eq43g4p",
        "outputId": "6b046cb1-bbfc-4909-adcf-bb83a4403f0d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "MobileNetV2\n",
        "Ventajas principales:\n",
        "Modelo ligero y eficiente: MobileNetV2 est√° dise√±ado para ser r√°pido y liviano, ideal si us√°s Colab, entornos con GPU limitada o pens√°s implementarlo en dispositivos m√≥viles o edge.\n",
        "Alto rendimiento: A pesar de ser liviano, alcanza buena precisi√≥n en tareas de clasificaci√≥n.\n",
        "Transfer learning efectivo: Funciona muy bien con fine-tuning para tareas con pocos datos (como en muchos proyectos de tesis).\n",
        "‚úÖ Justificaci√≥n para tu tesis:\n",
        "MobileNetV2 se eligi√≥ por su eficiencia computacional y alto rendimiento en clasificaci√≥n de im√°genes, siendo especialmente √∫til en contextos con recursos limitados o para aplicaciones en tiempo real. Adem√°s, su arquitectura permite realizar transfer learning de forma efectiva.\n",
        "\n",
        "\n",
        "\n",
        "üì• ¬øPor qu√© pretrained=True?\n",
        "Significado:\n",
        "Carga pesos ya entrenados en el dataset ImageNet (m√°s de 1M de im√°genes en 1000 clases).\n",
        "Esto significa que las primeras capas del modelo ya est√°n optimizadas para detectar caracter√≠sticas visuales gen√©ricas como bordes, texturas, patrones, etc.\n",
        "Ventajas:\n",
        "Mejora la convergencia (entrena m√°s r√°pido).\n",
        "Mejora la precisi√≥n, especialmente si ten√©s poco volumen de datos.\n",
        "Es la base del Transfer Learning.\n",
        "Justificaci√≥n para tu tesis:\n",
        "Se utilizan pesos preentrenados sobre ImageNet para aprovechar caracter√≠sticas visuales gen√©ricas aprendidas previamente, acelerando el entrenamiento y mejorando el rendimiento con un volumen de datos moderado.\n",
        "\n",
        "\n",
        "¬øPor qu√© CrossEntropyLoss?\n",
        "¬øQu√© hace?\n",
        "Es la funci√≥n de p√©rdida est√°ndar para clasificaci√≥n multi-clase (incluye binaria).\n",
        "Combina LogSoftmax + Negative Log Likelihood, calculando la discrepancia entre las probabilidades predichas y la clase verdadera.\n",
        "Justificaci√≥n:\n",
        "Es adecuada cuando las etiquetas est√°n codificadas como enteros (0 o 1).\n",
        "Autom√°ticamente maneja logits (sin necesidad de aplicar softmax manual).\n",
        "Justificaci√≥n para tu tesis:\n",
        "Se utiliza la funci√≥n de p√©rdida CrossEntropyLoss, ampliamente aceptada para problemas de clasificaci√≥n, ya que penaliza las predicciones incorrectas proporcionalmente a su confianza, incentivando al modelo a asignar probabilidades m√°s altas a las clases correctas.\n",
        "\n",
        "\n",
        "\n",
        "‚öôÔ∏è ¬øPor qu√© Adam y lr=0.001?\n",
        "\n",
        "¬øQu√© es Adam?\n",
        "Un optimizador adaptativo que ajusta la tasa de aprendizaje individualmente para cada peso.\n",
        "Combina ventajas de:\n",
        "RMSProp (ajuste por hist√≥rico de gradientes)\n",
        "Momentum (aceleraci√≥n del aprendizaje)\n",
        "¬øPor qu√© lr = 0.001?\n",
        "Es el valor por defecto y recomendado para Adam.\n",
        "Proporciona un balance entre estabilidad y velocidad en la mayor√≠a de los casos.\n",
        "Si lo aument√°s (por ej. 0.01):\n",
        "El modelo puede aprender m√°s r√°pido pero tambi√©n volverse inestable.\n",
        "Si lo baj√°s (por ej. 0.0001):\n",
        "El modelo aprende m√°s lento, pero m√°s establemente (√∫til si ten√©s overfitting o datos ruidosos).\n",
        "Justificaci√≥n para tu tesis:\n",
        "Se utiliza el optimizador Adam por su capacidad adaptativa y eficiencia, permitiendo un entrenamiento m√°s r√°pido y estable sin necesidad de ajustes manuales constantes. La tasa de aprendizaje inicial (lr = 0.001) es un valor est√°ndar que ofrece un equilibrio entre velocidad de convergencia y estabilidad num√©rica.\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "4F6zkDIs4CZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öôÔ∏è Detectar el dispositivo: GPU si est√° disponible, si no usa CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# üß† Mover el modelo al dispositivo seleccionado\n",
        "model = model.to(device)\n",
        "\n",
        "# üìä Calcular la distribuci√≥n de clases en el dataset de entrenamiento\n",
        "labels = np.array(train_dataset.labels)\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print(f\"Distribuci√≥n de clases en entrenamiento: {dict(zip(unique, counts))}\")\n",
        "\n",
        "# üßÆ Calcular pesos inversamente proporcionales a la cantidad de muestras por clase\n",
        "total = counts.sum()\n",
        "weights = [total / c for c in counts]  # M√°s peso a la clase menos representada\n",
        "\n",
        "# Convertir a tensor y mover a GPU (si est√° disponible)\n",
        "weights_tensor = torch.FloatTensor(weights).to(device)\n",
        "print(\"Pesos de clases:\", weights_tensor)\n",
        "\n",
        "# üéØ Definir una funci√≥n de p√©rdida ponderada para tratar el desbalance de clases\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJdVqM864oJ0",
        "outputId": "019a68ca-8d6a-498b-a900-6ff8260bfb48"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribuci√≥n de clases en entrenamiento: {np.int64(0): np.int64(17123), np.int64(1): np.int64(17123)}\n",
            "Pesos de clases: tensor([2., 2.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "El dataset de entrenamiento puede estar desbalanceado, es decir, tener m√°s ejemplos de una clase que de otra. Para evitar que el modelo aprenda a favorecer la clase mayoritaria, se calcula la distribuci√≥n real de clases y se asignan pesos inversamente proporcionales a su frecuencia. Estos pesos se utilizan en la funci√≥n CrossEntropyLoss para penalizar m√°s fuertemente los errores en las clases menos representadas. Esto mejora la equidad del modelo y su capacidad de detectar ambas clases por igual.\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
        "Con pesos seg√∫n distribuci√≥n\n",
        "Aplica mayor penalizaci√≥n a la clase minoritaria.\n",
        "Espec√≠ficamente √∫til si tu problema tiene desbalance de clases.\n",
        "Mejora m√©tricas como recall, F1-score macro y balanced accuracy.\n",
        "</details>"
      ],
      "metadata": {
        "id": "ZSlqAyV56O_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import sklearn.metrics as metrics\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# üîÅ Funci√≥n para entrenar una sola √©poca usando Mixed Precision\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device, scaler):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ‚öôÔ∏è Entrenamiento en precisi√≥n mixta (float16 + float32)\n",
        "        with autocast():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# üìä Funci√≥n para evaluar el modelo en validaci√≥n y obtener m√©tricas\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_corrects = 0\n",
        "    total_samples = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_corrects += torch.sum(preds == labels.data)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = val_loss / total_samples\n",
        "    epoch_acc = val_corrects.double() / total_samples\n",
        "    report = metrics.classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
        "\n",
        "    return epoch_loss, epoch_acc, report, all_preds, all_labels\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# üöÄ Funci√≥n principal de entrenamiento con validaci√≥n, checkpoints, early stopping y logging\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device,\n",
        "                num_epochs=10, patience=3, checkpoint_interval=1, log_dir=None):\n",
        "\n",
        "    if log_dir is None:\n",
        "        log_dir = os.path.join(RUN_DIR, \"tensorboard\")\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "    # üìù Guardar configuraci√≥n\n",
        "    config = {\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"patience\": patience,\n",
        "        \"checkpoint_interval\": checkpoint_interval,\n",
        "        \"batch_size\": train_loader.batch_size,\n",
        "        \"optimizer\": type(optimizer).__name__,\n",
        "        \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
        "    }\n",
        "    with open(os.path.join(RUN_DIR, \"config.json\"), \"w\") as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    best_val_loss = float('inf')\n",
        "    trigger_times = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}\\n' + '-' * 10)\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
        "        val_loss, val_acc, report, all_preds, all_labels = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        # para modificar el learning rate dinamicamente\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Mostrar el LR actual\n",
        "        for i, param_group in enumerate(optimizer.param_groups):\n",
        "            print(f\"üîÅ Learning Rate actual: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "        print(f'Train Loss: {train_loss:.4f}  Acc: {train_acc:.4f}')\n",
        "        print(f'Val Loss:   {val_loss:.4f}  Acc: {val_acc:.4f}')\n",
        "        print(metrics.classification_report(all_labels, all_preds, target_names=[\"spoof\", \"live\"], zero_division=0))\n",
        "\n",
        "        # TensorBoard logs\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "        writer.add_scalar('Acc/train', train_acc, epoch)\n",
        "        writer.add_scalar('Acc/val', val_acc, epoch)\n",
        "        writer.add_scalar('F1/macro_val', report['macro avg']['f1-score'], epoch)\n",
        "\n",
        "        # Early stopping + mejor modelo\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            trigger_times = 0\n",
        "            best_model_path = os.path.join(RUN_DIR, \"best_model.pth\")\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f'‚úÖ Best model saved to {best_model_path}')\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "            print(f'Early stopping trigger: {trigger_times}/{patience}')\n",
        "            if trigger_times >= patience:\n",
        "                print(\"‚õî Early stopping!\")\n",
        "                break\n",
        "\n",
        "        # Checkpoint por √©poca\n",
        "        if (epoch + 1) % checkpoint_interval == 0:\n",
        "            checkpoint_path = os.path.join(RUN_DIR, f'checkpoint_epoch_{epoch+1}.pth')\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'train_acc': train_acc.item(),\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc.item(),\n",
        "            }, checkpoint_path)\n",
        "            print(f'üì¶ Checkpoint saved to {checkpoint_path}')\n",
        "\n",
        "    writer.close()\n",
        "    return model, train_loss, train_acc, val_loss, val_acc, report\n",
        "\n"
      ],
      "metadata": {
        "id": "62VUKKYL6vcy"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><strong>üìö Explicaci√≥n detallada de funciones de entrenamiento</strong></summary>\n",
        "\n",
        "### üîÅ `train_epoch(...)`\n",
        "Esta funci√≥n entrena el modelo durante una sola √©poca usando *Mixed Precision Training* para mejorar el rendimiento y reducir el consumo de memoria en GPU.\n",
        "\n",
        "**Par√°metros:**\n",
        "- `model`: modelo de red neuronal.\n",
        "- `train_loader`: `DataLoader` con los datos de entrenamiento.\n",
        "- `optimizer`: optimizador (por ejemplo, Adam).\n",
        "- `criterion`: funci√≥n de p√©rdida.\n",
        "- `device`: CPU o GPU.\n",
        "- `scaler`: `GradScaler` para entrenamiento en precisi√≥n mixta.\n",
        "\n",
        "**Qu√© hace:**\n",
        "- Activa el modo entrenamiento del modelo.\n",
        "- Para cada batch:\n",
        "  - Ejecuta forward pass (predicci√≥n).\n",
        "  - Calcula la p√©rdida.\n",
        "  - Ejecuta backward pass y actualiza pesos (gradientes escalados).\n",
        "  - Acumula m√©tricas: p√©rdida total y aciertos.\n",
        "\n",
        "**Devuelve:**\n",
        "- `epoch_loss`: p√©rdida media de la √©poca.\n",
        "- `epoch_acc`: exactitud (accuracy) de entrenamiento.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä `validate_epoch(...)`\n",
        "Eval√∫a el modelo en el conjunto de validaci√≥n y calcula m√©tricas detalladas.\n",
        "\n",
        "**Par√°metros:**\n",
        "- `model`: modelo entrenado.\n",
        "- `val_loader`: datos de validaci√≥n.\n",
        "- `criterion`: funci√≥n de p√©rdida.\n",
        "- `device`: CPU o GPU.\n",
        "\n",
        "**Qu√© hace:**\n",
        "- Activa el modo evaluaci√≥n del modelo.\n",
        "- Desactiva c√°lculo de gradientes.\n",
        "- Calcula p√©rdida, predicciones y m√©tricas en todo el conjunto.\n",
        "- Genera un `classification_report` con precisi√≥n, recall y F1.\n",
        "\n",
        "**Devuelve:**\n",
        "- `epoch_loss`: p√©rdida media.\n",
        "- `epoch_acc`: exactitud (accuracy).\n",
        "- `report`: diccionario con m√©tricas detalladas (`f1-score`, `precision`, etc.).\n",
        "- `all_preds`: predicciones crudas.\n",
        "- `all_labels`: etiquetas verdaderas.\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ `train_model(...)`\n",
        "Funci√≥n principal de entrenamiento que combina las anteriores. Agrega:\n",
        "- Registro en TensorBoard.\n",
        "- Early Stopping.\n",
        "- Guardado de checkpoints y del mejor modelo.\n",
        "\n",
        "**Par√°metros:**\n",
        "- `model`: red neuronal.\n",
        "- `train_loader`, `val_loader`: `DataLoader`s para entrenamiento y validaci√≥n.\n",
        "- `criterion`: funci√≥n de p√©rdida.\n",
        "- `optimizer`: algoritmo de optimizaci√≥n.\n",
        "- `device`: CPU o GPU.\n",
        "- `num_epochs`: total de √©pocas.\n",
        "- `patience`: √©pocas sin mejora antes de detener.\n",
        "- `checkpoint_interval`: cada cu√°ntas √©pocas guardar.\n",
        "- `log_dir`: carpeta de logs de TensorBoard.\n",
        "\n",
        "**Qu√© hace:**\n",
        "- Itera por √©poca:\n",
        "  - Entrena una √©poca (`train_epoch`)\n",
        "  - Valida la √©poca (`validate_epoch`)\n",
        "  - Registra m√©tricas en TensorBoard\n",
        "  - Guarda el mejor modelo\n",
        "  - Aplica Early Stopping si no mejora\n",
        "  - Guarda checkpoints peri√≥dicos\n",
        "\n",
        "**Devuelve:**\n",
        "- El modelo final entrenado.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "yIRhXM-i70N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# continua del bloque anterior, pero lo pongo aca para tenerlo separado\n",
        "# üß™ Ejecuci√≥n del entrenamiento (se asume que todo est√° definido)\n",
        "trained_model, train_loss, train_acc, val_loss, val_acc, val_report = train_model(\n",
        "    model, train_loader, val_loader,\n",
        "    criterion, optimizer, device,\n",
        "    num_epochs=10, patience=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NjeBDJpTUQQ",
        "outputId": "f59b20b7-a4f5-4bf5-c918-c80c9b19d77e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-70-2456198354.py:95: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-70-2456198354.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/tmp/ipython-input-70-2456198354.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Learning Rate actual: 0.000500\n",
            "Train Loss: 0.0388  Acc: 0.9856\n",
            "Val Loss:   0.0673  Acc: 0.9756\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       spoof       1.00      0.96      0.98      6507\n",
            "        live       0.94      1.00      0.97      3493\n",
            "\n",
            "    accuracy                           0.98     10000\n",
            "   macro avg       0.97      0.98      0.97     10000\n",
            "weighted avg       0.98      0.98      0.98     10000\n",
            "\n",
            "‚úÖ Best model saved to /content/drive/MyDrive/Tesis UTDT/experimentos/20250622-221825/best_model.pth\n",
            "üì¶ Checkpoint saved to /content/drive/MyDrive/Tesis UTDT/experimentos/20250622-221825/checkpoint_epoch_1.pth\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-70-2456198354.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/tmp/ipython-input-70-2456198354.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Learning Rate actual: 0.000500\n",
            "Train Loss: 0.0140  Acc: 0.9952\n",
            "Val Loss:   0.0723  Acc: 0.9760\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       spoof       1.00      0.96      0.98      6507\n",
            "        live       0.94      1.00      0.97      3493\n",
            "\n",
            "    accuracy                           0.98     10000\n",
            "   macro avg       0.97      0.98      0.97     10000\n",
            "weighted avg       0.98      0.98      0.98     10000\n",
            "\n",
            "Early stopping trigger: 1/3\n",
            "üì¶ Checkpoint saved to /content/drive/MyDrive/Tesis UTDT/experimentos/20250622-221825/checkpoint_epoch_2.pth\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-70-2456198354.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/tmp/ipython-input-70-2456198354.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Learning Rate actual: 0.000250\n",
            "Train Loss: 0.0122  Acc: 0.9957\n",
            "Val Loss:   0.0695  Acc: 0.9761\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       spoof       1.00      0.96      0.98      6507\n",
            "        live       0.94      1.00      0.97      3493\n",
            "\n",
            "    accuracy                           0.98     10000\n",
            "   macro avg       0.97      0.98      0.97     10000\n",
            "weighted avg       0.98      0.98      0.98     10000\n",
            "\n",
            "Early stopping trigger: 2/3\n",
            "üì¶ Checkpoint saved to /content/drive/MyDrive/Tesis UTDT/experimentos/20250622-221825/checkpoint_epoch_3.pth\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-70-2456198354.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/tmp/ipython-input-70-2456198354.py:52: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÅ Learning Rate actual: 0.000250\n",
            "Train Loss: 0.0058  Acc: 0.9983\n",
            "Val Loss:   0.0805  Acc: 0.9734\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       spoof       1.00      0.96      0.98      6507\n",
            "        live       0.93      1.00      0.96      3493\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.96      0.98      0.97     10000\n",
            "weighted avg       0.98      0.97      0.97     10000\n",
            "\n",
            "Early stopping trigger: 3/3\n",
            "‚õî Early stopping!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn.metrics as metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# üß† Cargar el mejor modelo desde el directorio de la corrida actual\n",
        "best_model = models.mobilenet_v2(pretrained=False)\n",
        "num_ftrs = best_model.classifier[1].in_features\n",
        "best_model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
        "best_model.load_state_dict(torch.load(os.path.join(RUN_DIR, 'best_model.pth')))\n",
        "best_model = best_model.to(device)\n",
        "print(\"‚úÖ Mejor modelo cargado desde RUN_DIR\")\n",
        "\n",
        "\n",
        "\n",
        "def test_model(model, test_loader, criterion, device, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    test_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Usar autocast para precisi√≥n mixta en la inferencia\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            total_samples += inputs.size(0)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = test_loss / total_samples\n",
        "    print(f'Test Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Convertir a arrays de numpy\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Calcular matriz de confusi√≥n y reporte en formato dict\n",
        "    cm = metrics.confusion_matrix(all_labels, all_preds)\n",
        "    report = metrics.classification_report(all_labels, all_preds, target_names=[\"spoof\", \"live\"], zero_division=0)\n",
        "    report_dict = metrics.classification_report(all_labels, all_preds, target_names=[\"spoof\", \"live\"], output_dict=True, zero_division=0)\n",
        "\n",
        "    # Guardar m√©tricas como JSON\n",
        "    with open(os.path.join(output_dir, 'classification_report.json'), 'w') as f:\n",
        "        json.dump(report, f, indent=4)\n",
        "    print(\"üìÑ Reporte de clasificaci√≥n guardado en JSON.\")\n",
        "\n",
        "    print(\"Matriz de Confusi√≥n:\")\n",
        "    print(cm)\n",
        "    print(\"Reporte de Clasificaci√≥n:\")\n",
        "    print(report)\n",
        "\n",
        "    # Guardar matriz de confusi√≥n como imagen PNG\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"spoof\", \"live\"], yticklabels=[\"spoof\", \"live\"])\n",
        "    plt.xlabel(\"Predicci√≥n\")\n",
        "    plt.ylabel(\"Real\")\n",
        "    plt.title(\"Matriz de Confusi√≥n\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'))\n",
        "    plt.close()\n",
        "    print(\"üñºÔ∏è Matriz de confusi√≥n guardada como imagen.\")\n",
        "\n",
        "    return avg_loss, report_dict, cm\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH9KuuHz72Uf",
        "outputId": "20b5c3b2-d454-4f00-ced0-14e7088666c7"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mejor modelo cargado desde RUN_DIR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><strong>üß™ Evaluaci√≥n del modelo sobre el conjunto de test</strong></summary>\n",
        "\n",
        "Esta etapa eval√∫a el modelo entrenado sobre un conjunto independiente de prueba. La funci√≥n `test_model` se encarga de calcular:\n",
        "\n",
        "- La p√©rdida promedio (`test_loss`)\n",
        "- M√©tricas de clasificaci√≥n como:\n",
        "  - Accuracy\n",
        "  - Precision\n",
        "  - Recall\n",
        "  - F1-score (por clase y macro)\n",
        "- La matriz de confusi√≥n\n",
        "- Una visualizaci√≥n gr√°fica con `seaborn`\n",
        "\n",
        "### üîç Detalles t√©cnicos:\n",
        "- El modelo se eval√∫a en modo `eval()` y sin c√°lculo de gradientes (`no_grad()`).\n",
        "- Se usa `autocast()` para inferencia eficiente con precisi√≥n mixta (float16).\n",
        "- Las predicciones y etiquetas se acumulan para calcular m√©tricas globales con `sklearn.metrics`.\n",
        "\n",
        "### üìä Resultado esperado:\n",
        "- Impresi√≥n por consola del `classification_report`.\n",
        "- Visualizaci√≥n clara de errores y aciertos con una matriz de confusi√≥n coloreada.\n",
        "- Retorno de:\n",
        "  - `avg_loss`: p√©rdida total dividida por cantidad de muestras.\n",
        "  - `report`: diccionario con todas las m√©tricas.\n",
        "  - `cm`: matriz de confusi√≥n en formato NumPy.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "npG9kS0k8tDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuacion del bloque anterior. Testeo del modelo en otro dataset\n",
        "test_data_dir2 = '/content/drive/Othercomputers/My Mac/Data/test'\n",
        "\n",
        "test_dataset2 = CelebASpoofDataset(\n",
        "    root_dir=test_data_dir2,\n",
        "    transform=data_transforms['test'],\n",
        "    cache_file=os.path.join(CACHE_DIR, 'test_dataset_cache_full.pkl'),\n",
        "    max_samples=10000\n",
        "    # max_subjects=300\n",
        ")\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "\n",
        "# Ejemplo de uso:\n",
        "# test_loss, test_report, test_cm = test_model(model, test_loader2, criterion, device)\n",
        "test_loss, test_report, test_cm = test_model(best_model, test_loader2, criterion, device, output_dir=RUN_DIR)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGz2agcmUXdn",
        "outputId": "ec6e10ba-be7a-46a4-ddf7-07551abe12dd"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando dataset desde cache...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-72-328312363.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.5762\n",
            "üìÑ Reporte de clasificaci√≥n guardado en JSON.\n",
            "Matriz de Confusi√≥n:\n",
            "[[1353 3972]\n",
            " [   7 4668]]\n",
            "Reporte de Clasificaci√≥n:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       spoof       0.99      0.25      0.40      5325\n",
            "        live       0.54      1.00      0.70      4675\n",
            "\n",
            "    accuracy                           0.60     10000\n",
            "   macro avg       0.77      0.63      0.55     10000\n",
            "weighted avg       0.78      0.60      0.54     10000\n",
            "\n",
            "üñºÔ∏è Matriz de confusi√≥n guardada como imagen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def log_experiment_summary(run_dir,\n",
        "                           train_loss, train_acc,\n",
        "                           val_loss, val_acc, val_report,\n",
        "                           test_loss, test_report,\n",
        "                           train_max_samples, val_max_samples, test_max_samples):\n",
        "    summary_path = os.path.join(EXPERIMENTS_DIR, \"all_runs.csv\")\n",
        "    run_name = os.path.basename(run_dir)\n",
        "\n",
        "    # Utilidad segura para extraer m√©tricas, aunque una clase no est√© presente\n",
        "    def get_metric(report, class_name, metric):\n",
        "        return round(report.get(class_name, {}).get(metric, 0.0), 4)\n",
        "\n",
        "    summary_data = {\n",
        "        \"run_name\": run_name,\n",
        "\n",
        "        # M√©tricas de entrenamiento\n",
        "        \"train_loss\": round(train_loss, 4),\n",
        "        \"train_acc\": round(train_acc.item(), 4),\n",
        "\n",
        "        # M√©tricas de validaci√≥n\n",
        "        \"val_loss\": round(val_loss, 4),\n",
        "        \"val_acc\": round(val_acc.item(), 4),\n",
        "        \"f1_macro_val\": get_metric(val_report, \"macro avg\", \"f1-score\"),\n",
        "        \"precision_spoof_val\": get_metric(val_report, \"0\", \"precision\"),\n",
        "        \"recall_spoof_val\": get_metric(val_report, \"0\", \"recall\"),\n",
        "        \"f1_spoof_val\": get_metric(val_report, \"0\", \"f1-score\"),\n",
        "        \"precision_live_val\": get_metric(val_report, \"1\", \"precision\"),\n",
        "        \"recall_live_val\": get_metric(val_report, \"1\", \"recall\"),\n",
        "        \"f1_live_val\": get_metric(val_report, \"1\", \"f1-score\"),\n",
        "\n",
        "        # M√©tricas de test final\n",
        "        \"test_loss\": round(test_loss, 4),\n",
        "        \"f1_macro_test\": get_metric(test_report, \"macro avg\", \"f1-score\"),\n",
        "        \"precision_spoof_test\": get_metric(test_report, \"spoof\", \"precision\"),\n",
        "        \"recall_spoof_test\": get_metric(test_report, \"spoof\", \"recall\"),\n",
        "        \"f1_spoof_test\": get_metric(test_report, \"spoof\", \"f1-score\"),\n",
        "        \"precision_live_test\": get_metric(test_report, \"live\", \"precision\"),\n",
        "        \"recall_live_test\": get_metric(test_report, \"live\", \"recall\"),\n",
        "        \"f1_live_test\": get_metric(test_report, \"live\", \"f1-score\"),\n",
        "\n",
        "        # Info de dataset\n",
        "        \"train_max_samples\": train_max_samples,\n",
        "        \"val_max_samples\": val_max_samples,\n",
        "        \"test_max_samples\": test_max_samples\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', header=False, index=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"üìù Resumen de la corrida guardado en {summary_path}\")\n"
      ],
      "metadata": {
        "id": "XW8B2HBXX5-M"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAsTob5fZ_iW",
        "outputId": "e1d17fe2-14c8-438d-94b4-9067758f7a2e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'spoof': {'precision': 0.9948529411764706,\n",
              "  'recall': 0.2540845070422535,\n",
              "  'f1-score': 0.40478683620044875,\n",
              "  'support': 5325.0},\n",
              " 'live': {'precision': 0.5402777777777777,\n",
              "  'recall': 0.9985026737967915,\n",
              "  'f1-score': 0.7011641006383778,\n",
              "  'support': 4675.0},\n",
              " 'accuracy': 0.6021,\n",
              " 'macro avg': {'precision': 0.7675653594771241,\n",
              "  'recall': 0.6262935904195225,\n",
              "  'f1-score': 0.5529754684194133,\n",
              "  'support': 10000.0},\n",
              " 'weighted avg': {'precision': 0.7823390522875817,\n",
              "  'recall': 0.6021,\n",
              "  'f1-score': 0.5433432073251805,\n",
              "  'support': 10000.0}}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_experiment_summary(\n",
        "    run_dir=RUN_DIR,\n",
        "    train_loss=train_loss,\n",
        "    train_acc=train_acc,\n",
        "    val_loss=val_loss,\n",
        "    val_acc=val_acc,\n",
        "    val_report=val_report,\n",
        "    test_loss=test_loss,\n",
        "    test_report=test_report,\n",
        "    train_max_samples=train_max_samples,\n",
        "    val_max_samples=val_max_samples,\n",
        "    test_max_samples=test_max_samples\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE0EMB-aYERA",
        "outputId": "6743577a-a937-4e30-fc21-424153291e02"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Resumen de la corrida guardado en /content/drive/MyDrive/Tesis UTDT/experimentos/all_runs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar 10 predicciones incorrectas con su path\n",
        "for i, (img, label) in enumerate(test_dataset):\n",
        "    pred = model(img.unsqueeze(0).to(device)).argmax(dim=1).item()\n",
        "    if pred != label:\n",
        "        print(f\"ERROR ‚Üí Pred: {pred} | Real: {label} | Path: {test_dataset.image_paths[i]}\")\n",
        "        if i > 10: break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBXaRogcnMSE",
        "outputId": "a50fbf53-54eb-4796-8513-78caa796ae6f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR ‚Üí Pred: 1 | Real: 0 | Path: /content/drive/Othercomputers/My Mac/Data/train/test/9893/spoof/192739.jpg\n",
            "ERROR ‚Üí Pred: 1 | Real: 0 | Path: /content/drive/Othercomputers/My Mac/Data/train/test/9893/spoof/179815.jpg\n"
          ]
        }
      ]
    }
  ]
}